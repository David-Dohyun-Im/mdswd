---
lesson_id: "01-modern-foundations"
lesson_title: "Modern Software Foundations"
source_dir: "mswd/01-modern-foundations"
step_count: 10
---

<!-- STEP:01 -->
## Step 01: Understanding the 2025 Software Landscape

# Understanding the 2025 Software Landscape

Welcome to the course! Before we dive into tooling, take a breath and look around. In 2025 you’re building software in the middle of an AI supercycle: headlines about AGI appear daily, funding is abundant, and expectations are sky-high. At the same time CS enrollments are down 20% and software jobs are in flux. The lesson is that you won’t be replaced by AI—you’ll be replaced by someone who knows how to wield it with taste.

In this course we replace “vibe coding” (blindly tab-accepting suggestions) with **human-agent engineering**. That means:

- Pairing deep systems thinking with AI acceleration
- Carrying business context like a tech lead
- Reading copious amounts of code to build taste
- Experimenting aggressively because the playbook is still being written

Keep that mindset front and center: AI is leverage, not a substitute for judgment.

## Step Quiz
**Questions**
1) Why is "human-agent engineering" emphasized over "vibe coding"?
**Answers**
1) Because robust software still requires humans who understand systems, context, and taste, and AI serves as leverage rather than a drop-in replacement.

<!-- STEP:02 -->
## Step 02: How LLMs Learn to Code

# How LLMs Learn to Code

Let’s demystify the engines powering modern workflows. Large language models are autoregressive next-token predictors built on transformers, so every decision comes from continuing a probability distribution over tokens. Training happens in three big stages:

1. **Self-supervised pretraining** on hundreds of billions of tokens (Common Crawl, Wikipedia, GitHub) to learn raw language and code statistics.
2. **Supervised finetuning** using curated prompt-response pairs so the model learns to follow human instructions.
3. **Preference (alignment) tuning** like RLHF or DPO to nudge answers toward helpful, harmless, and high-quality outputs.

Code-first models (Codex, Code LLaMA, StarCoder) simply add massive code corpora during stage one. When you give a clean, contextual prompt, you’re helping the model replay the best patterns it has seen.

## Step Quiz
**Questions**
1) What are the three broad training stages modern LLMs undergo?
**Answers**
1) Self-supervised pretraining, supervised finetuning, and preference/reward tuning for alignment.

<!-- STEP:03 -->
## Step 03: Prompting Basics—Zero-Shot and Few-Shot

# Prompting Basics—Zero-Shot and Few-Shot

Prompts are how you “program” the model. Two foundational techniques:

- **Zero-shot prompting**: give instructions with no extra scaffolding. Use this for common patterns the model likely knows (simple React components, small Python utilities).
- **K-shot prompting**: add a few examples that demonstrate naming, formatting, or domain-specific APIs. This shines when your repo has bespoke abstractions the model has never seen.

Your job is to pick the right tool. Too many examples can overconstrain the LLM and waste context, while too few leave it guessing. Think like a teacher: show just enough to anchor the behavior you want.

## Step Quiz
**Questions**
1) When is K-shot prompting especially useful?
**Answers**
1) When guiding an LLM through domain-specific APIs or coding conventions it likely has not seen in pretraining.

<!-- STEP:04 -->
## Step 04: Prompting for Reasoning

# Prompting for Reasoning

Complex engineering tasks rarely fit into one hop. Chain-of-Thought (CoT) prompting forces the model to articulate intermediate steps before producing code. You can:

- Provide multi-shot examples that include reasoning traces
- Use zero-shot CoT with phrases like “Let’s think step by step” or wrap reasoning in `<reasoning>` tags

This is invaluable for math, debugging, or multi-hop architecture questions because it slows the model down and exposes its logic. You can even pair CoT with self-consistency by sampling multiple reasoning paths and comparing answers.

## Step Quiz
**Questions**
1) Why does Chain-of-Thought prompting improve difficult coding or math tasks?
**Answers**
1) It forces the LLM to surface intermediate reasoning steps, leading to more reliable multi-hop logic before producing code.

<!-- STEP:05 -->
## Step 05: Driving Toward Reliable Outputs

# Driving Toward Reliable Outputs

Once you ask the model to reason, you still need to verify the result. A practical technique is **self-consistency**: sample several responses (often 5) with the same prompt and take the most common answer. Incorrect logic tends to diverge, while correct logic converges. You can also instrument prompts with requests for evidence (“Reference filenames when diagnosing errors”) or explicit checkpoints (“Ask clarifying questions before editing files”).

Treat prompt design like engineering: add guardrails, collect signals, and automate what works.

## Step Quiz
**Questions**
1) How does self-consistency prompting reduce hallucinations?
**Answers**
1) By sampling multiple reasoning traces and taking the most common answer, it filters out errant one-off hallucinations.

<!-- STEP:06 -->
## Step 06: Building a Coding Agent Loop

# Building a Coding Agent Loop

Now apply your prompting skills to an agent. At its core, a coding agent loop does the following:

1. Shows the system prompt (behavioral rules)
2. Lists available tools (`read_file`, `list_dir`, `edit_file`, …)
3. Receives a user request
4. Lets the LLM decide whether to respond directly or call a tool
5. Executes the tool offline, then feeds the result back as context
6. Repeats until the task is done

This is the same skeleton running inside Cursor, Windsurf, and Claude Code. Your differentiators are better instructions, richer memory, and higher-fidelity tool descriptions.

## Step Quiz
**Questions**
1) Why does an agent need to enumerate its available tools up front?
**Answers**
1) Because the LLM decides when to call tools based on the list it receives in-context; without it the agent cannot plan interactions with the environment.

<!-- STEP:07 -->
## Step 07: Secret Sauce from Production Agents

# Secret Sauce from Production Agents

Commercial agents survive because they obsess over context hygiene. Claude, for instance, inserts `<system-reminder>` tags everywhere—system prompts, tool calls, even tool results—to keep behavior anchored as transcripts grow. It prefaces shell commands with explicit instructions, extracts command prefixes for determinism, and spawns subagents with their own prompts when tasks get large. Treat these patterns as your toolbox: don’t rely on one giant prompt, sprinkle lightweight reminders and structure throughout the conversation.

## Step Quiz
**Questions**
1) What purpose do repeated system reminders serve inside agent transcripts?
**Answers**
1) They continually restate behavioral constraints so the LLM does not drift as new tool output floods the context window.

<!-- STEP:08 -->
## Step 08: Why Model Context Protocol Matters

# Why Model Context Protocol Matters

Your agent can only act on the context it sees. Before MCP, every new tool required bespoke glue code, authentication flows, and formatter prompts. Model Context Protocol standardizes the handshake:

- Hosts request a tools catalog from each server
- Servers reply with JSON schemas describing each tool
- Hosts inject those schemas into the model so it can issue structured calls

Now “What’s the Bitcoin price?” and “Summarize my Gmail” look identical to the agent—it just chooses a tool whose schema matches the request. MCP turns one-off integrations into reusable building blocks.

## Step Quiz
**Questions**
1) What core pain does MCP solve for agent builders?
**Answers**
1) It replaces bespoke integrations with a standardized way to describe and call tools, so you avoid building custom connectors for every service.

<!-- STEP:09 -->
## Step 09: MCP Architecture in Practice

# MCP Architecture in Practice

Let’s map the pieces:

1. **Host** (Cursor, Claude Desktop) embeds an `MCPClient`.
2. **Server** wraps a resource (Gmail, filesystem, Jira) and exposes endpoints like `tools/list`.
3. **Tool** is a JSON-described function the model can call.

When a session starts, the host calls `tools/list`, injects the schemas into the LLM, and waits for structured tool calls. Servers execute the call, stream back results over stdio or SSE, and the loop continues. MCP even supports proactive workflows where servers send notifications without waiting for user prompts.

## Step Quiz
**Questions**
1) Which component provides the tool catalog the model sees?
**Answers**
1) The MCP server responds to `tools/list` with JSON describing each callable tool, which the host then injects into the model’s context.

<!-- STEP:10 -->
## Step 10: Designing for MCP Limitations

# Designing for MCP Limitations

Finally, remember that MCP isn’t limitless. Agents still struggle if you expose dozens of tools with verbose schemas—the context window fills up and tooling becomes unusable. Design AI-native interfaces:

- Keep JSON schemas concise and deterministic
- Group related actions into focused servers
- Document auth flows and rate limits in plain language
- Return consistent error messages the model can parse

If a junior engineer would be confused by the API, an LLM will be too. Treat MCP tooling like product design: simple, predictable, and well-documented.

## Step Quiz
**Questions**
1) Why should MCP tools be “AI-native” rather than raw wrappers over legacy APIs?
**Answers**
1) Because concise, predictable schemas consume less context, reduce confusion, and give the model a realistic chance of calling the tool correctly.
