---
lesson_id: "01-modern-foundations"
lesson_title: "Modern Software Foundations"
source_dir: "mswd/01-modern-foundations"
step_count: 10
---

<!-- STEP:01 -->
## Step 01: Understanding the 2025 Software Landscape

# Understanding the 2025 Software Landscape

Welcome to the course! Before we dive into tooling, take a breath and look around. In 2025 you’re building software in the middle of an AI supercycle: headlines about AGI appear daily, funding is abundant, and expectations are sky-high. At the same time CS enrollments are down 20% and software jobs are in flux. The lesson is that you won’t be replaced by AI—you’ll be replaced by someone who knows how to wield it with taste.

In this course we replace “vibe coding” (blindly tab-accepting suggestions) with **human-agent engineering**. That means:

- Pairing deep systems thinking with AI acceleration
- Carrying business context like a tech lead
- Reading copious amounts of code to build taste
- Experimenting aggressively because the playbook is still being written

Keep that mindset front and center: AI is leverage, not a substitute for judgment.

## Step Quiz
**Questions**
1) Why is "human-agent engineering" emphasized over "vibe coding"?
**Answers**
1) Because robust software still requires humans who understand systems, context, and taste, and AI serves as leverage rather than a drop-in replacement.

<!-- STEP:02 -->
## Step 02: How LLMs Learn to Code

# How LLMs Learn to Code

Let’s demystify the engines powering modern workflows. Large language models are autoregressive next-token predictors built on transformers, so every decision comes from continuing a probability distribution over tokens. Training happens in three big stages:

1. **Self-supervised pretraining** on hundreds of billions of tokens (Common Crawl, Wikipedia, GitHub) to learn raw language and code statistics.
2. **Supervised finetuning** using curated prompt-response pairs so the model learns to follow human instructions.
3. **Preference (alignment) tuning** like RLHF or DPO to nudge answers toward helpful, harmless, and high-quality outputs.

Code-first models (Codex, Code LLaMA, StarCoder) simply add massive code corpora during stage one. When you give a clean, contextual prompt, you’re helping the model replay the best patterns it has seen.

## Step Quiz
**Questions**
1) What are the three broad training stages modern LLMs undergo?
**Answers**
1) Self-supervised pretraining, supervised finetuning, and preference/reward tuning for alignment.

<!-- STEP:03 -->
## Step 03: Prompting Basics—Zero-Shot and Few-Shot

# Prompting Basics—Zero-Shot and Few-Shot

Prompts are how you “program” the model. Two foundational techniques:

- **Zero-shot prompting**: give instructions with no extra scaffolding. Use this for common patterns the model likely knows (simple React components, small Python utilities).
- **K-shot prompting**: add a few examples that demonstrate naming, formatting, or domain-specific APIs. This shines when your repo has bespoke abstractions the model has never seen.

Your job is to pick the right tool. Too many examples can overconstrain the LLM and waste context, while too few leave it guessing. Think like a teacher: show just enough to anchor the behavior you want.

**Coding exercise – Week 1 K-Shot Prompt.** Edit `modern-software-dev-assignments/week1/k_shot_prompting.py` by filling `YOUR_SYSTEM_PROMPT` with a pattern that shows the model how to reverse text. For example:

```python
# week1/k_shot_prompting.py
YOUR_SYSTEM_PROMPT = """
You are a meticulous string utility. Learn from the examples:
Input: desk -> Output: ksed
Input: Radar -> Output: radaR
Always return only the reversed string.
"""
```

Run `python week1/k_shot_prompting.py` until the `EXPECTED_OUTPUT` check passes. This mirrors the drill-style coding snippets in the original course.

## Step Quiz
**Questions**
1) When is K-shot prompting especially useful?
2) Which starter file should you edit to practice K-shot prompting?
**Answers**
1) When guiding an LLM through domain-specific APIs or coding conventions it likely has not seen in pretraining.
2) `week1/k_shot_prompting.py`.

<!-- STEP:04 -->
## Step 04: Prompting for Reasoning

# Prompting for Reasoning

Complex engineering tasks rarely fit into one hop. Chain-of-Thought (CoT) prompting forces the model to articulate intermediate steps before producing code. You can:

- Provide multi-shot examples that include reasoning traces
- Use zero-shot CoT with phrases like “Let’s think step by step” or wrap reasoning in `<reasoning>` tags

This is invaluable for math, debugging, or multi-hop architecture questions because it slows the model down and exposes its logic. You can even pair CoT with self-consistency by sampling multiple reasoning paths and comparing answers.

Starter script (`week1/chain_of_thought.py`):

```python
import os
import re
from dotenv import load_dotenv
from ollama import chat

load_dotenv()

NUM_RUNS_TIMES = 5

# TODO: Fill this in!
YOUR_SYSTEM_PROMPT = ""


USER_PROMPT = """
Solve this problem, then give the final answer on the last line as "Answer: <number>".

what is 3^{12345} (mod 100)?
"""


# For this simple example, we expect the final numeric answer only
EXPECTED_OUTPUT = "Answer: 43"


def extract_final_answer(text: str) -> str:
    """Extract the final 'Answer: ...' line from a verbose reasoning trace.

    - Finds the LAST line that starts with 'Answer:' (case-insensitive)
    - Normalizes to 'Answer: <number>' when a number is present
    - Falls back to returning the matched content if no number is detected
    """
    matches = re.findall(r"(?mi)^\s*answer\s*:\s*(.+)\s*$", text)
    if matches:
        value = matches[-1].strip()
        # Prefer a numeric normalization when possible (supports integers/decimals)
        num_match = re.search(r"-?\d+(?:\.\d+)?", value.replace(",", ""))
        if num_match:
            return f"Answer: {num_match.group(0)}"
        return f"Answer: {value}"
    return text.strip()


def test_your_prompt(system_prompt: str) -> bool:
    """Run up to NUM_RUNS_TIMES and return True if any output matches EXPECTED_OUTPUT.

    Prints "SUCCESS" when a match is found.
    """
    for idx in range(NUM_RUNS_TIMES):
        print(f"Running test {idx + 1} of {NUM_RUNS_TIMES}")
        response = chat(
            model="llama3.1:8b",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": USER_PROMPT},
            ],
            options={"temperature": 0.3},
        )
        output_text = response.message.content
        final_answer = extract_final_answer(output_text)
        if final_answer.strip() == EXPECTED_OUTPUT.strip():
            print("SUCCESS")
            return True
        else:
            print(f"Expected output: {EXPECTED_OUTPUT}")
            print(f"Actual output: {final_answer}")
    return False


if __name__ == "__main__":
    test_your_prompt(YOUR_SYSTEM_PROMPT)
```

**Coding exercise – Chain-of-thought drill.** Open `week1/chain_of_thought.py` and craft a reasoning-heavy `YOUR_SYSTEM_PROMPT`. A typical solution reminds the model to show work and end with `Answer: <number>`:

```python
YOUR_SYSTEM_PROMPT = """
You are a careful mathematician. Always reason step-by-step in bullet lists,
double-check modular arithmetic, and end with 'Answer: <number>'.
"""
```

Run `python week1/chain_of_thought.py` to verify that at least one of the `NUM_RUNS_TIMES` outputs matches the expected answer.

## Step Quiz
**Questions**
1) Why does Chain-of-Thought prompting improve difficult coding or math tasks?
2) In which file do you enable CoT for the “3^12345 mod 100” drill?
**Answers**
1) It forces the LLM to surface intermediate reasoning steps, leading to more reliable multi-hop logic before producing code.
2) `week1/chain_of_thought.py`.

<!-- STEP:05 -->
## Step 05: Driving Toward Reliable Outputs

# Driving Toward Reliable Outputs

Once you ask the model to reason, you still need to verify the result. A practical technique is **self-consistency**: sample several responses (often 5) with the same prompt and take the most common answer. Incorrect logic tends to diverge, while correct logic converges. You can also instrument prompts with requests for evidence (“Reference filenames when diagnosing errors”) or explicit checkpoints (“Ask clarifying questions before editing files”).

Treat prompt design like engineering: add guardrails, collect signals, and automate what works.

Starter script (`week1/self_consistency_prompting.py`):

```python
import os
import re
from collections import Counter
from dotenv import load_dotenv
from ollama import chat

load_dotenv()

NUM_RUNS_TIMES = 5

# TODO: Fill this in! Try to get as close to 100% correctness across all runs as possible.
YOUR_SYSTEM_PROMPT = ""

USER_PROMPT = """
Solve this problem, then give the final answer on the last line as "Answer: <number>".

Henry made two stops during his 60-mile bike trip. He first stopped after 20
miles. His second stop was 15 miles before the end of the trip. How many miles
did he travel between his first and second stops?
"""

EXPECTED_OUTPUT = "Answer: 25"


def extract_final_answer(text: str) -> str:
    """Extract the final 'Answer: ...' line from a verbose reasoning trace.

    - Finds the LAST line that starts with 'Answer:' (case-insensitive)
    - Normalizes to 'Answer: <number>' when a number is present
    - Falls back to returning the matched content if no number is detected
    """
    matches = re.findall(r"(?mi)^\s*answer\s*:\s*(.+)\s*$", text)
    if matches:
        value = matches[-1].strip()
        num_match = re.search(r"-?\d+(?:\.\d+)?", value.replace(",", ""))
        if num_match:
            return f"Answer: {num_match.group(0)}"
        return f"Answer: {value}"
    return text.strip()


def test_your_prompt(system_prompt: str) -> bool:
    """Run the prompt NUM_RUNS_TIMES, majority-vote on the extracted 'Answer: ...' lines.

    Prints "SUCCESS" if the majority answer equals EXPECTED_OUTPUT.
    """
    answers: list[str] = []
    for idx in range(NUM_RUNS_TIMES):
        print(f"Running test {idx + 1} of {NUM_RUNS_TIMES}")
        response = chat(
            model="llama3.1:8b",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": USER_PROMPT},
            ],
            options={"temperature": 1},
        )
        output_text = response.message.content
        final_answer = extract_final_answer(output_text)
        print(f"Run {idx + 1} answer: {final_answer}")
        answers.append(final_answer.strip())

    if not answers:
        print("No answers produced.")
        return False

    counts = Counter(answers)
    majority_answer, majority_count = counts.most_common(1)[0]
    print(f"Majority answer: {majority_answer} ({majority_count}/{len(answers)})")

    if majority_answer.strip() == EXPECTED_OUTPUT.strip():
        print("SUCCESS")
        return True

    # Print distribution for debugging when majority does not match expected
    print(f"Expected output: {EXPECTED_OUTPUT}")
    print("Answer distribution:")
    for answer, count in counts.most_common():
        print(f"  {answer}: {count}")
    return False


if __name__ == "__main__":
    test_your_prompt(YOUR_SYSTEM_PROMPT)
```

**Coding exercise – Self-consistency prompting.** Implement majority voting inside `week1/self_consistency_prompting.py`. Fill `YOUR_SYSTEM_PROMPT` with guidance that encourages explicit reasoning and consistent formatting:

```python
YOUR_SYSTEM_PROMPT = """
You are a math tutor. Explain the distance traveled between checkpoints with numbered steps.
Return the final number as 'Answer: <number>' on its own line.
"""
```

Run `python week1/self_consistency_prompting.py` and watch the `Counter` output converge on the correct majority answer.

## Step Quiz
**Questions**
1) How does self-consistency prompting reduce hallucinations?
2) Where in the starter repo do you implement the majority-vote loop?
**Answers**
1) By sampling multiple reasoning traces and taking the most common answer, it filters out errant one-off hallucinations.
2) `week1/self_consistency_prompting.py`.

<!-- STEP:06 -->
## Step 06: Building a Coding Agent Loop

# Building a Coding Agent Loop

Now apply your prompting skills to an agent. At its core, a coding agent loop does the following:

1. Shows the system prompt (behavioral rules)
2. Lists available tools (`read_file`, `list_dir`, `edit_file`, …)
3. Receives a user request
4. Lets the LLM decide whether to respond directly or call a tool
5. Executes the tool offline, then feeds the result back as context
6. Repeats until the task is done

This is the same skeleton running inside Cursor, Windsurf, and Claude Code. Your differentiators are better instructions, richer memory, and higher-fidelity tool descriptions.

Starter script (`week1/tool_calling.py`):

```python
import ast
import json
import os
from typing import Any, Dict, List, Optional, Tuple, Callable

from dotenv import load_dotenv
from ollama import chat

load_dotenv()

NUM_RUNS_TIMES = 3


# ==========================
# Tool implementation (the "executor")
# ==========================
def _annotation_to_str(annotation: Optional[ast.AST]) -> str:
    if annotation is None:
        return "None"
    try:
        return ast.unparse(annotation)  # type: ignore[attr-defined]
    except Exception:
        # Fallback best-effort
        if isinstance(annotation, ast.Name):
            return annotation.id
        return type(annotation).__name__


def _list_function_return_types(file_path: str) -> List[Tuple[str, str]]:
    with open(file_path, "r", encoding="utf-8") as f:
        source = f.read()
    tree = ast.parse(source)
    results: List[Tuple[str, str]] = []
    for node in tree.body:
        if isinstance(node, ast.FunctionDef):
            return_str = _annotation_to_str(node.returns)
            results.append((node.name, return_str))
    # Sort for stable output
    results.sort(key=lambda x: x[0])
    return results


def output_every_func_return_type(file_path: str = None) -> str:
    """Tool: Return a newline-delimited list of "name: return_type" for each top-level function."""
    path = file_path or __file__
    if not os.path.isabs(path):
        # Try file relative to this script if not absolute
        candidate = os.path.join(os.path.dirname(__file__), path)
        if os.path.exists(candidate):
            path = candidate
    pairs = _list_function_return_types(path)
    return "\n".join(f"{name}: {ret}" for name, ret in pairs)


# Sample functions to ensure there is something to analyze
def add(a: int, b: int) -> int:
    return a + b


def greet(name: str) -> str:
    return f"Hello, {name}!"

# Tool registry for dynamic execution by name
TOOL_REGISTRY: Dict[str, Callable[..., str]] = {
    "output_every_func_return_type": output_every_func_return_type,
}

# ==========================
# Prompt scaffolding
# ==========================

# TODO: Fill this in!
YOUR_SYSTEM_PROMPT = ""


def resolve_path(p: str) -> str:
    if os.path.isabs(p):
        return p
    here = os.path.dirname(__file__)
    c1 = os.path.join(here, p)
    if os.path.exists(c1):
        return c1
    # Try sibling of project root if needed
    return p


def extract_tool_call(text: str) -> Dict[str, Any]:
    """Parse a single JSON object from the model output."""
    text = text.strip()
    # Some models wrap JSON in code fences; attempt to strip
    if text.startswith("```") and text.endswith("```"):
        text = text.strip("`")
        if text.lower().startswith("json\n"):
            text = text[5:]
    try:
        obj = json.loads(text)
        return obj
    except json.JSONDecodeError:
        raise ValueError("Model did not return valid JSON for the tool call")


def run_model_for_tool_call(system_prompt: str) -> Dict[str, Any]:
    response = chat(
        model="llama3.1:8b",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "Call the tool now."},
        ],
        options={"temperature": 0.3},
    )
    content = response.message.content
    return extract_tool_call(content)


def execute_tool_call(call: Dict[str, Any]) -> str:
    name = call.get("tool")
    if not isinstance(name, str):
        raise ValueError("Tool call JSON missing 'tool' string")
    func = TOOL_REGISTRY.get(name)
    if func is None:
        raise ValueError(f"Unknown tool: {name}")
    args = call.get("args", {})
    if not isinstance(args, dict):
        raise ValueError("Tool call JSON 'args' must be an object")

    # Best-effort path resolution if a file_path arg is present
    if "file_path" in args and isinstance(args["file_path"], str):
        args["file_path"] = resolve_path(args["file_path"]) if str(args["file_path"]) != "" else __file__
    elif "file_path" not in args:
        # Provide default for tools expecting file_path
        args["file_path"] = __file__

    return func(**args)


def compute_expected_output() -> str:
    # Ground-truth expected output based on the actual file contents
    return output_every_func_return_type(__file__)


def test_your_prompt(system_prompt: str) -> bool:
    """Run once: require the model to produce a valid tool call; compare tool output to expected."""
    expected = compute_expected_output()
    for _ in range(NUM_RUNS_TIMES):
        try:
            call = run_model_for_tool_call(system_prompt)
        except Exception as exc:
            print(f"Failed to parse tool call: {exc}")
            continue
        print(call)
        try:
            actual = execute_tool_call(call)
        except Exception as exc:
            print(f"Tool execution failed: {exc}")
            continue
        if actual.strip() == expected.strip():
            print(f"Generated tool call: {call}")
            print(f"Generated output: {actual}")
            print("SUCCESS")
            return True
        else:
            print("Expected output:\n" + expected)
            print("Actual output:\n" + actual)
    return False


if __name__ == "__main__":
    test_your_prompt(YOUR_SYSTEM_PROMPT)
```

**Coding exercise – Tool calling practice.** The scaffolding in `week1/tool_calling.py` already includes a registry and AST tool. Your job is to finish `YOUR_SYSTEM_PROMPT` so the model produces valid JSON tool calls:

```python
YOUR_SYSTEM_PROMPT = """
You can call tools by emitting JSON like {"tool": "output_every_func_return_type", "args": {"file_path": "chain_of_thought.py"}}.
First reason about the file you want to inspect, then output ONLY the JSON object.
"""
```

Execute `python week1/tool_calling.py` and confirm that the parsed JSON triggers `output_every_func_return_type`.

## Step Quiz
**Questions**
1) Why does an agent need to enumerate its available tools up front?
2) Which function registry in the starter repo should you extend when adding more tool calls?
**Answers**
1) Because the LLM decides when to call tools based on the list it receives in-context; without it the agent cannot plan interactions with the environment.
2) `TOOL_REGISTRY` inside `week1/tool_calling.py`.

<!-- STEP:07 -->
## Step 07: Secret Sauce from Production Agents

# Secret Sauce from Production Agents

Commercial agents survive because they obsess over context hygiene. Claude, for instance, inserts `<system-reminder>` tags everywhere—system prompts, tool calls, even tool results—to keep behavior anchored as transcripts grow. It prefaces shell commands with explicit instructions, extracts command prefixes for determinism, and spawns subagents with their own prompts when tasks get large. Treat these patterns as your toolbox: don’t rely on one giant prompt, sprinkle lightweight reminders and structure throughout the conversation.

Starter script (`week1/reflexion.py`):

```python
import os
import re
from typing import Callable, List, Tuple
from dotenv import load_dotenv
from ollama import chat

load_dotenv()

NUM_RUNS_TIMES = 1

SYSTEM_PROMPT = """
You are a coding assistant. Output ONLY a single fenced Python code block that defines
the function is_valid_password(password: str) -> bool. No prose or comments.
Keep the implementation minimal.
"""

# TODO: Fill this in!
YOUR_REFLEXION_PROMPT = ""


# Ground-truth test suite used to evaluate generated code
SPECIALS = set("!@#$%^&*()-_")
TEST_CASES: List[Tuple[str, bool]] = [
    ("Password1!", True),       # valid
    ("password1!", False),      # missing uppercase
    ("Password!", False),       # missing digit
    ("Password1", False),       # missing special
]


def extract_code_block(text: str) -> str:
    m = re.findall(r"```python\n([\s\S]*?)```", text, flags=re.IGNORECASE)
    if m:
        return m[-1].strip()
    m = re.findall(r"```\n([\s\S]*?)```", text)
    if m:
        return m[-1].strip()
    return text.strip()


def load_function_from_code(code_str: str) -> Callable[[str], bool]:
    namespace: dict = {}
    exec(code_str, namespace)  # noqa: S102 (executing controlled code from model for exercise)
    func = namespace.get("is_valid_password")
    if not callable(func):
        raise ValueError("No callable is_valid_password found in generated code")
    return func


def evaluate_function(func: Callable[[str], bool]) -> Tuple[bool, List[str]]:
    failures: List[str] = []
    for pw, expected in TEST_CASES:
        try:
            result = bool(func(pw))
        except Exception as exc:
            failures.append(f"Input: {pw} → raised exception: {exc}")
            continue

        if result != expected:
            # Compute diagnostic based on ground-truth rules
            reasons = []
            if len(pw) < 8:
                reasons.append("length < 8")
            if not any(c.islower() for c in pw):
                reasons.append("missing lowercase")
            if not any(c.isupper() for c in pw):
                reasons.append("missing uppercase")
            if not any(c.isdigit() for c in pw):
                reasons.append("missing digit")
            if not any(c in SPECIALS for c in pw):
                reasons.append("missing special")
            if any(c.isspace() for c in pw):
                reasons.append("has whitespace")

            failures.append(
                f"Input: {pw} → expected {expected}, got {result}. Failing checks: {', '.join(reasons) or 'unknown'}"
            )

    return (len(failures) == 0, failures)


def generate_initial_function(system_prompt: str) -> str:
    response = chat(
        model="llama3.1:8b",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "Provide the implementation now."},
        ],
        options={"temperature": 0.2},
    )
    return extract_code_block(response.message.content)


def your_build_reflexion_context(prev_code: str, failures: List[str]) -> str:
    """TODO: Build the user message for the reflexion step using prev_code and failures.

    Return a string that will be sent as the user content alongside the reflexion system prompt.
    """
    return ""


def apply_reflexion(
    reflexion_prompt: str,
    build_context: Callable[[str, List[str]], str],
    prev_code: str,
    failures: List[str],
) -> str:
    reflection_context = build_context(prev_code, failures)
    print(f"REFLECTION CONTEXT: {reflection_context}, {reflexion_prompt}")
    response = chat(
        model="llama3.1:8b",
        messages=[
            {"role": "system", "content": reflexion_prompt},
            {"role": "user", "content": reflection_context},
        ],
        options={"temperature": 0.2},
    )
    return extract_code_block(response.message.content)


def run_reflexion_flow(
    system_prompt: str,
    reflexion_prompt: str,
    build_context: Callable[[str, List[str]], str],
) -> bool:
    # 1) Generate initial function
    initial_code = generate_initial_function(system_prompt)
    print("Initial code:\n" + initial_code)
    func = load_function_from_code(initial_code)
    passed, failures = evaluate_function(func)
    if passed:
        print("SUCCESS (initial implementation passed all tests)")
        return True
    else:
        print(f"FAILURE (initial implementation failed some tests): {failures}")

    # 2) Single reflexion iteration
    improved_code = apply_reflexion(reflexion_prompt, build_context, initial_code, failures)
    print("\nImproved code:\n" + improved_code)
    improved_func = load_function_from_code(improved_code)
    passed2, failures2 = evaluate_function(improved_func)
    if passed2:
        print("SUCCESS")
        return True

    print("Tests still failing after reflexion:")
    for f in failures2:
        print("- " + f)
    return False


if __name__ == "__main__":
    run_reflexion_flow(SYSTEM_PROMPT, YOUR_REFLEXION_PROMPT, your_build_reflexion_context)
```

**Coding exercise – Reflexion loop.** Practice context hygiene by finishing the reflexion loop in `week1/reflexion.py`. Implement `YOUR_REFLEXION_PROMPT` and `your_build_reflexion_context()` so the model reasons over failing tests:

```python
YOUR_REFLEXION_PROMPT = """
You review failing Python password validators. Read the previous code and failures, then output a corrected implementation.
Return only a fenced python block.
"""

def your_build_reflexion_context(prev_code: str, failures: List[str]) -> str:
    failure_summary = "\n".join(f"- {f}" for f in failures)
    return (
        "Previous implementation:\n"
        f"```python\n{prev_code}\n```\n"
        "Fix the issues described below:\n"
        f"{failure_summary}\n"
    )
```

Run `python week1/reflexion.py` to iterate once and see how the improved prompt reduces drift.

## Step Quiz
**Questions**
1) What purpose do repeated system reminders serve inside agent transcripts?
2) Which reflexion helper builds the follow-up user message in the starter repo?
**Answers**
1) They continually restate behavioral constraints so the LLM does not drift as new tool output floods the context window.
2) `your_build_reflexion_context` inside `week1/reflexion.py`.

<!-- STEP:08 -->
## Step 08: Why Model Context Protocol Matters

# Why Model Context Protocol Matters

Your agent can only act on the context it sees. Before MCP, every new tool required bespoke glue code, authentication flows, and formatter prompts. Model Context Protocol standardizes the handshake:

- Hosts request a tools catalog from each server
- Servers reply with JSON schemas describing each tool
- Hosts inject those schemas into the model so it can issue structured calls

Now “What’s the Bitcoin price?” and “Summarize my Gmail” look identical to the agent—it just chooses a tool whose schema matches the request. MCP turns one-off integrations into reusable building blocks.

Starter RAG script (`week1/rag.py`):

```python
import os
import re
from typing import List, Callable
from dotenv import load_dotenv
from ollama import chat

load_dotenv()

NUM_RUNS_TIMES = 5

DATA_FILES: List[str] = [
    os.path.join(os.path.dirname(__file__), "data", "api_docs.txt"),
]


def load_corpus_from_files(paths: List[str]) -> List[str]:
    corpus: List[str] = []
    for p in paths:
        if os.path.exists(p):
            try:
                with open(p, "r", encoding="utf-8") as f:
                    corpus.append(f.read())
            except Exception as exc:
                corpus.append(f"[load_error] {p}: {exc}")
        else:
            corpus.append(f"[missing_file] {p}")
    return corpus


# Load corpus from external files (simple API docs). If missing, fall back to inline snippet
CORPUS: List[str] = load_corpus_from_files(DATA_FILES)

QUESTION = (
    "Write a Python function `fetch_user_name(user_id: str, api_key: str) -> str` that calls the documented API "
    "to fetch a user by id and returns only the user's name as a string."
)


# TODO: Fill this in!
YOUR_SYSTEM_PROMPT = ""


# For this simple example
# For this coding task, validate by required snippets rather than exact string
REQUIRED_SNIPPETS = [
    "def fetch_user_name(",
    "requests.get",
    "/users/",
    "X-API-Key",
    "return",
]


def YOUR_CONTEXT_PROVIDER(corpus: List[str]) -> List[str]:
    """TODO: Select and return the relevant subset of documents from CORPUS for this task.

    For example, return [] to simulate missing context, or [corpus[0]] to include the API docs.
    """
    return []


def make_user_prompt(question: str, context_docs: List[str]) -> str:
    if context_docs:
        context_block = "\n".join(f"- {d}" for d in context_docs)
    else:
        context_block = "(no context provided)"
    return (
        f"Context (use ONLY this information):\n{context_block}\n\n"
        f"Task: {question}\n\n"
        "Requirements:\n"
        "- Use the documented Base URL and endpoint.\n"
        "- Send the documented authentication header.\n"
        "- Raise for non-200 responses.\n"
        "- Return only the user's name string.\n\n"
        "Output: A single fenced Python code block with the function and necessary imports.\n"
    )


def extract_code_block(text: str) -> str:
    """Extract the last fenced Python code block, or any fenced code block, else return text."""
    # Try ```python ... ``` first
    m = re.findall(r"```python\n([\s\S]*?)```", text, flags=re.IGNORECASE)
    if m:
        return m[-1].strip()
    # Fallback to any fenced code block
    m = re.findall(r"```\n([\s\S]*?)```", text)
    if m:
        return m[-1].strip()
    return text.strip()


def test_your_prompt(system_prompt: str, context_provider: Callable[[List[str]], List[str]]) -> bool:
    """Run up to NUM_RUNS_TIMES and return True if any output matches EXPECTED_OUTPUT."""
    context_docs = context_provider(CORPUS)
    user_prompt = make_user_prompt(QUESTION, context_docs)

    for idx in range(NUM_RUNS_TIMES):
        print(f"Running test {idx + 1} of {NUM_RUNS_TIMES}")
        response = chat(
            model="llama3.1:8b",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            options={"temperature": 0.0},
        )
        output_text = response.message.content
        code = extract_code_block(output_text)
        missing = [s for s in REQUIRED_SNIPPETS if s not in code]
        if not missing:
            print(output_text)
            print("SUCCESS")
            return True
        else:
            print("Missing required snippets:")
            for s in missing:
                print(f"  - {s}")
            print("Generated code:\n" + code)
    return False


if __name__ == "__main__":
    test_your_prompt(YOUR_SYSTEM_PROMPT, YOUR_CONTEXT_PROVIDER)
```

**Coding exercise – RAG as proto-MCP.** `week1/rag.py` simulates bringing external docs into the model’s context. Implement `YOUR_SYSTEM_PROMPT` and `YOUR_CONTEXT_PROVIDER` so the function reads API docs from `data/api_docs.txt`:

```python
YOUR_SYSTEM_PROMPT = """
You answer only using the provided API documentation. Output a fenced python block with imports plus fetch_user_name implementation.
"""

def YOUR_CONTEXT_PROVIDER(corpus: List[str]) -> List[str]:
    return [corpus[0]]  # ensure the API docs file is injected
```

Running `python week1/rag.py` gives you a mini version of how MCP supplies structured context.

## Step Quiz
**Questions**
1) What core pain does MCP solve for agent builders?
2) Which RAG helper function do you edit to control which docs enter the LLM context?
**Answers**
1) It replaces bespoke integrations with a standardized way to describe and call tools, so you avoid building custom connectors for every service.
2) `YOUR_CONTEXT_PROVIDER` inside `week1/rag.py`.

<!-- STEP:09 -->
## Step 09: MCP Architecture in Practice

# MCP Architecture in Practice

Let’s map the pieces:

1. **Host** (Cursor, Claude Desktop) embeds an `MCPClient`.
2. **Server** wraps a resource (Gmail, filesystem, Jira) and exposes endpoints like `tools/list`.
3. **Tool** is a JSON-described function the model can call.

When a session starts, the host calls `tools/list`, injects the schemas into the LLM, and waits for structured tool calls. Servers execute the call, stream back results over stdio or SSE, and the loop continues. MCP even supports proactive workflows where servers send notifications without waiting for user prompts.

**Coding exercise – Week 3 server skeleton.** In `week3/`, scaffold a simple MCP server such as `week3/server/main.py`:

```python
import httpx
from mcp.server.fastmcp import FastMCPServer

server = FastMCPServer("notes-api")

@server.tool()
async def fetch_note(note_id: int) -> dict:
    """Fetch a note from the starter FastAPI backend."""
    async with httpx.AsyncClient() as client:
        resp = await client.get(f"http://localhost:8000/api/notes/{note_id}")
        resp.raise_for_status()
        return resp.json()

if __name__ == "__main__":
    server.run()
```

Wire this into Claude Desktop or Cursor using the MCP quickstart so you can test `tools/list` locally.

## Step Quiz
**Questions**
1) Which component provides the tool catalog the model sees?
2) Where should you add your MCP entrypoint when starting the Week 3 assignment?
**Answers**
1) The MCP server responds to `tools/list` with JSON describing each callable tool, which the host then injects into the model’s context.
2) Create something like `week3/server/main.py` that instantiates your MCP server.

<!-- STEP:10 -->
## Step 10: Designing for MCP Limitations

# Designing for MCP Limitations

Finally, remember that MCP isn’t limitless. Agents still struggle if you expose dozens of tools with verbose schemas—the context window fills up and tooling becomes unusable. Design AI-native interfaces:

- Keep JSON schemas concise and deterministic
- Group related actions into focused servers
- Document auth flows and rate limits in plain language
- Return consistent error messages the model can parse

If a junior engineer would be confused by the API, an LLM will be too. Treat MCP tooling like product design: simple, predictable, and well-documented.

**Coding exercise – Describe tool schemas.** Extend the skeleton above with explicit schemas so the agent knows what to send:

```python
@server.tool()
async def search_notes(query: str) -> dict:
    """
    summary: "Search notes by keyword"
    parameters:
      query:
        type: string
        description: "Text to match in title or body"
    """
    ...
```

Document both tools in `week3/README.md`, including auth expectations if your upstream API requires `X-API-Key`.

## Step Quiz
**Questions**
1) Why should MCP tools be “AI-native” rather than raw wrappers over legacy APIs?
2) What document should you update with tool schemas and auth instructions when finishing Week 3?
**Answers**
1) Because concise, predictable schemas consume less context, reduce confusion, and give the model a realistic chance of calling the tool correctly.
2) `week3/README.md`, so humans and agents share the same contract.
Starter script from `week1/k_shot_prompting.py` (copy it here if you can’t open the repo):

```python
import os
from dotenv import load_dotenv
from ollama import chat

load_dotenv()

NUM_RUNS_TIMES = 5

# TODO: Fill this in!
YOUR_SYSTEM_PROMPT = ""

USER_PROMPT = """
Reverse the order of letters in the following word. Only output the reversed word, no other text:

httpstatus
"""


EXPECTED_OUTPUT = "sutatsptth"

def test_your_prompt(system_prompt: str) -> bool:
    """Run the prompt up to NUM_RUNS_TIMES and return True if any output matches EXPECTED_OUTPUT.

    Prints "SUCCESS" when a match is found.
    """
    for idx in range(NUM_RUNS_TIMES):
        print(f"Running test {idx + 1} of {NUM_RUNS_TIMES}")
        response = chat(
            model="mistral-nemo:12b",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": USER_PROMPT},
            ],
            options={"temperature": 0.5},
        )
        output_text = response.message.content.strip()
        if output_text.strip() == EXPECTED_OUTPUT.strip():
            print("SUCCESS")
            return True
        else:
            print(f"Expected output: {EXPECTED_OUTPUT}")
            print(f"Actual output: {output_text}")
    return False

if __name__ == "__main__":
    test_your_prompt(YOUR_SYSTEM_PROMPT)
```
