---
lesson_id: "03-ai-product-trends"
lesson_title: "AI Product Trends and App Building"
source_dir: "mswd/03-ai-product-trends"
step_count: 10
---

<!-- STEP:01 -->
## Step 01: Programming at an Inflection Point

# Programming at an Inflection Point

Start by zooming out. Boris Cherny argues that both programming languages and IDE productivity sit on exponential curves. We‚Äôve moved from punch cards ‚Üí FORTRAN ‚Üí C ‚Üí Python ‚Üí AI copilots ‚Üí Devin in a few decades, and the slope is steepening. His advice: **think six months ahead**. Assume the model will be smarter and compute cheaper by the time your product ships, so design workflows that stay relevant instead of chasing last quarter‚Äôs capability.

## Step Quiz
**Questions**
1) Why does Boris emphasize ‚Äúthink six months out‚Äù when building AI dev tools?
**Answers**
1) Because model quality and compute costs are improving rapidly, so products must anticipate near-term leaps instead of chasing yesterday‚Äôs capabilities.

<!-- STEP:02 -->
## Step 02: Claude Code Everywhere

# Claude Code Everywhere

Claude Code ships where developers already live:

- Terminal-native CLI
- IDE integrations
- Web and mobile apps
- GitHub App
- SDK for custom workflows

Ask ‚ÄúWhat did I ship this week?‚Äù directly from your shell, or correlate GCP logs via a piped command. The same agent brain powers each surface so you avoid context switching and keep a consistent workflow in the terminal, the editor, or your phone.

**Coding exercise ‚Äì CLI-based code analysis.** Practice using AI from the command line. First, create a simple FastAPI application `api.py`:

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

class Note(BaseModel):
    id: int
    title: str
    content: str

@app.get("/")
async def root():
    return {"message": "Notes API"}

@app.get("/notes")
async def list_notes():
    return {"notes": []}

@app.post("/notes")
async def create_note(note: Note):
    return {"created": note}

@app.get("/notes/{note_id}")
async def get_note(note_id: int):
    if note_id < 1:
        raise HTTPException(status_code=404, detail="Note not found")
    return {"id": note_id, "title": "Sample", "content": "Test"}
```

If you have Claude CLI installed, try analyzing it:

```bash
# Analyze the file
claude -p "List every FastAPI route in api.py and describe what each does." \
  --allowedTools ReadFile(api.py)

# Or use any AI CLI tool
# llm "List all FastAPI routes in api.py"
# aichat "Analyze api.py routes"
```

Document the output to practice CLI-based code exploration.

## Step Quiz
**Questions**
1) What advantage does offering Claude Code across terminal, IDE, web, and SDKs provide?
2) What should you analyze with CLI-based AI tools?
**Answers**
1) Developers can invoke the same agentic workflows wherever they are, reducing context switching between tooling surfaces.
2) Route structures, function signatures, code complexity, or any quick analysis that doesn't require deep modification.

<!-- STEP:03 -->
## Step 03: Core Use Cases and Tooling

# Core Use Cases and Tooling

The product gravitates around four use cases:

1. **Codebase Q&A** ‚Äì Ask about files, history, or why a fix landed.
2. **Write code** ‚Äì Choose between one-shot edits, sidekick pair-programming, or full prototype loops.
3. **Integrate tools** ‚Äì Add MCP servers or CLI wrappers so the agent can call internal systems (‚ÄúUse the Barley CLI to check logs‚Äù).
4. **Automate workflows** ‚Äì Chain tasks across CI/CD, screenshots, or verification steps.

You decide how much control to give the agent, just like assigning work to a teammate.

**Coding exercise ‚Äì Add an LLM-powered endpoint.** Extend your FastAPI app with an AI-powered summarization endpoint:

```python
from ollama import chat

@app.post("/summarize")
async def summarize_text(text: str):
    """Summarize text using local LLM."""
    try:
        response = chat(
            model="llama3.1:8b",
            messages=[
                {"role": "system", "content": "You summarize text concisely in 2-3 sentences."},
                {"role": "user", "content": text}
            ]
        )
        return {
            "original_length": len(text),
            "summary": response.message.content,
            "model": "llama3.1:8b"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Summarization failed: {str(e)}")
```

Document the endpoint in `API.md`:

```markdown
# API Documentation

## POST /summarize

Summarizes input text using a local LLM.

### Request
```json
{
  "text": "Long text to summarize..."
}
```

### Response
```json
{
  "original_length": 500,
  "summary": "Brief summary...",
  "model": "llama3.1:8b"
}
```

### Error Handling
- 500: LLM service unavailable or processing error
```

## Step Quiz
**Questions**
1) Why does Claude Code support multiple "write code" modes (one-shot, sidekick, prototype)?
2) What information should API documentation include?
**Answers**
1) Different tasks need different levels of autonomy and iteration, from quick edits to extended prototype loops with UI previews.
2) Endpoint path, request/response formats, parameters, error codes, and example usage.

<!-- STEP:04 -->
## Step 04: Designing for Flexible Workflows

# Designing for Flexible Workflows

Different problems deserve different cadences. Claude Code demonstrates workflow templates such as:

- **Explore ‚Üí plan ‚Üí confirm ‚Üí code ‚Üí commit** for risky bug hunts.
- **Tests ‚Üí code ‚Üí iterate ‚Üí commit** when leading with TDD.
- **Code ‚Üí screenshot ‚Üí iterate** for UI polish.

Commands encode these stages so you can choose the path that matches the task‚Äôs risk profile. Don‚Äôt force everything through a single chat loop.

**Coding exercise ‚Äì Define a workflow template.** Create `WORKFLOWS.md` to document standard procedures:

```markdown
# Development Workflows

## Workflow: API Documentation Refresh

Use when API endpoints change.

### Steps
1. **Test First**
   ```bash
   pytest tests/test_api.py -v
   ```

2. **Generate Docs**
   - Run docstring extractor
   - Update OpenAPI schema
   - Sync README examples

3. **Review Changes**
   - Check for breaking changes
   - Update version numbers if needed
   - Verify examples still work

4. **Commit**
   ```bash
   git add docs/ README.md
   git commit -m "docs: update API documentation"
   ```

---

## Workflow: Feature Implementation (TDD)

Use for new features with clear requirements.

### Steps
1. **Write Tests**
   - Create test file: `tests/test_feature.py`
   - Write failing tests for all requirements
   - Run tests: `pytest tests/test_feature.py`

2. **Implement**
   - Write minimal code to pass tests
   - Keep functions small and focused
   - Add type hints and docstrings

3. **Refactor**
   - Improve code readability
   - Extract common patterns
   - Run tests again to ensure nothing broke

4. **Quality Check**
   ```bash
   make lint
   make test
   make format
   ```

5. **Commit**
   ```bash
   git add .
   git commit -m "feat: implement [feature name]"
   ```
```

Reference these workflows when prompting AI agents to ensure consistent processes.

## Step Quiz
**Questions**
1) What is the benefit of explicitly encoding workflow templates?
2) When should you reference a workflow template?
**Answers**
1) It aligns the agent's behavior with the task's risk profile, ensuring the right checkpoints happen before code is merged.
2) When starting a complex task or when you want an AI agent to follow a specific sequence of steps consistently.

<!-- STEP:05 -->
## Step 05: Lessons from Claude Code

# Lessons from Claude Code

Boris closes with four lessons:

1. **Build for the next model** ‚Äì today‚Äôs ceiling becomes tomorrow‚Äôs floor.
2. **Stay adaptable** ‚Äì prompts, hooks, and tools will keep shifting.
3. **Ask what workflows need** ‚Äì design around human goals, not model tricks.
4. **Invest in verification** ‚Äì AI-powered tests, fuzzing, and self-play systems are already emerging.

Think of the agent as a partner across the entire SDLC, not just autocomplete.

**Coding exercise ‚Äì Generate project documentation.** Use an AI agent to create comprehensive documentation. Here's the prompt to use:

```
Analyze this codebase and create a README.md with:

1. Project Overview - what does this application do?
2. Architecture - key components and their relationships
3. Setup Instructions - how to install and run
4. API Endpoints - list all routes with descriptions
5. Testing - how to run tests
6. Development Workflow - common tasks for contributors

Be specific and include code examples where helpful.
```

Example generated README structure:

```markdown
# Notes API

A FastAPI-based note-taking application with LLM-powered features.

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FastAPI   ‚îÇ
‚îÇ   Routes    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Services   ‚îÇ
‚îÇ  (LLM)      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Database   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Start server
uvicorn main:app --reload
```

## API Endpoints

- `GET /` - Health check
- `GET /notes` - List all notes
- `POST /notes` - Create a new note
- `POST /summarize` - Summarize text using LLM

## Testing

```bash
pytest tests/ -v --cov
```

## Development

1. Write tests first (`tests/`)
2. Implement feature (`app/`)
3. Run quality checks (`make check`)
4. Commit changes
```

Save both your prompt and the generated README for reference.

## Step Quiz
**Questions**
1) What mindset shift does "ask not what the model can do for you" encourage?
2) What should you save along with AI-generated documentation?
**Answers**
1) To design products around reimagined workflows instead of bolting AI onto existing processes.
2) Save the prompt you used, so you can regenerate or improve the documentation later with similar instructions.

<!-- STEP:06 -->
## Step 06: Building Breakout Tools (Warp)

# Building Breakout Tools (Warp)

Warp‚Äôs seven principles are a playbook for AI-first dev tools:

- **Start with familiar UX** (terminal, IDE, chat) so people onboard instantly.
- **Align behind universal pain points** like cryptic `tar` flags.
- **Offer configuration flexibility**‚Äîzero-setup defaults plus deep power-user knobs.
- **Focus on ergonomics** and deliver ‚Äúfive minutes to wow.‚Äù
- **Make chat first-class** because code is just a proxy for intent.
- **Integrate MCP** to tap live data safely.
- **Create rapid feedback loops** so users see the impact immediately.

**Coding exercise ‚Äì Create a quality check script.** Build an automated workflow script for common development tasks:

```bash
#!/bin/bash
# quality-check.sh - Run all quality checks

set -e  # Exit on any error

echo "üéØ Starting quality checks..."

echo "üìù Step 1: Formatting code..."
black . --check || (echo "‚ùå Format issues found. Run: black ." && exit 1)

echo "üîç Step 2: Running linter..."
pylint **/*.py || (echo "‚ùå Lint issues found." && exit 1)

echo "üß™ Step 3: Running tests..."
pytest tests/ -v --cov --cov-report=term-missing || (echo "‚ùå Tests failed." && exit 1)

echo "‚úÖ All quality checks passed!"
echo ""
echo "Ready to commit!"
```

Make it executable and save it:

```bash
chmod +x quality-check.sh
./quality-check.sh
```

This creates a "five minutes to wow" experience - one command to verify everything works.

Document your script in `WORKFLOWS.md`:

```markdown
## Quick Quality Check

Run all checks at once:
```bash
./quality-check.sh
```

This runs formatting, linting, and tests in sequence.
```

## Step Quiz
**Questions**
1) Why does Warp prioritize "five minutes to wow"?
2) What's the benefit of a single quality-check script?
**Answers**
1) Early delight reduces onboarding friction and proves the agent adds value before users invest in deeper configuration.
2) It provides consistent, repeatable quality checks and reduces cognitive load - developers don't need to remember multiple commands.

<!-- STEP:07 -->
## Step 07: Agent Workflows and YOLO Mode

# Agent Workflows and YOLO Mode

Warp demonstrates multiple autonomy levels:

- **Copilot mode** ‚Äì ask clarifying questions, get inline help.
- **Guided automation** ‚Äì the agent proposes plans, you approve before execution.
- **YOLO mode** ‚Äì full autonomy where the agent takes the wheel end-to-end.

MCP brings a live data mesh (e.g., Braintrust context) into the terminal, and real-time panels show what‚Äôs happening. Expect the ecosystem to keep debating standard config files and how much consolidation will occur.

**Coding exercise ‚Äì Parallel development with Git worktrees.** Learn to isolate concurrent work streams using Git worktrees:

```bash
# Setup: Create a simple project with main branch
git init my-project
cd my-project
echo "# My Project" > README.md
git add . && git commit -m "initial commit"

# Create worktrees for parallel work
git worktree add ../my-project-feature-a -b feature-a
git worktree add ../my-project-feature-b -b feature-b

# Now you can work on three things simultaneously:
# Terminal 1: ./my-project (main branch)
# Terminal 2: ../my-project-feature-a (feature-a branch)
# Terminal 3: ../my-project-feature-b (feature-b branch)
```

Practice workflow:

1. **Terminal 1** (feature-a worktree):
   ```bash
   cd ../my-project-feature-a
   # Let AI agent implement authentication
   # Run: pytest tests/test_auth.py
   ```

2. **Terminal 2** (feature-b worktree):
   ```bash
   cd ../my-project-feature-b
   # Let AI agent implement data export
   # Run: pytest tests/test_export.py
   ```

3. **Original directory** - review and merge:
   ```bash
   cd my-project
   git merge feature-a  # After tests pass
   git merge feature-b  # After tests pass
   ```

Cleanup when done:
```bash
git worktree remove ../my-project-feature-a
git worktree remove ../my-project-feature-b
```

## Step Quiz
**Questions**
1) What differentiates "YOLO mode" from lighter chat interactions?
2) What's the main advantage of git worktrees?
**Answers**
1) YOLO mode gives the agent end-to-end autonomy to execute substantive tasks without constant human confirmation.
2) Multiple worktrees let you work on different features simultaneously without branch switching, ideal for running parallel AI agents.

<!-- STEP:08 -->
## Step 08: The Future of App Development (v0)

# The Future of App Development (v0)

Now shift to app builders. With Vercel‚Äôs v0 you:

1. Describe your UI in natural language.
2. Watch models generate production-grade React + TypeScript.
3. Preview live and iterate in real time.
4. Deploy with one click.

This unlocks collaboration: PMs validate ideas instantly, designers see their vision implemented, engineers focus on hard logic, and founders ship MVPs solo.

**Coding exercise ‚Äì Compare AI app builders.** Experiment with AI-powered app generation tools. Try building the same simple app with different tools:

**Project spec**: A note-taking app with:
- List view showing all notes
- Create new note form
- Edit existing notes
- Delete notes
- Search functionality

**Tools to try**:
1. **v0.dev** (React/Next.js)
2. **Bolt.new** (Choose your stack)
3. **Replit Agent** (Multiple languages)

For each generated app, create a folder and document:

```
generated-apps/
  v0-version/
    [generated files]
    README.md
  bolt-version/
    [generated files]
    README.md
  comparison.md
```

`comparison.md` template:

```markdown
# AI App Builder Comparison

## v0.dev
- **Time to first working app**: X minutes
- **Stack**: Next.js + Tailwind
- **Quality**: Rate 1-5
- **Manual fixes needed**: [list what you had to fix]
- **Pros**: ...
- **Cons**: ...

## Bolt.new
- **Time to first working app**: Y minutes
- **Stack**: [your choice]
- **Quality**: Rate 1-5
- **Manual fixes needed**: [list]
- **Pros**: ...
- **Cons**: ...

## Conclusion
[Which tool fit your workflow best? Why?]
```

## Step Quiz
**Questions**
1) How does v0 change collaboration between PMs, designers, engineers, and founders?
2) What should you track when comparing AI app builders?
**Answers**
1) It lets each persona express intent in natural language and immediately see production-quality code, reducing hand-offs and speeding validation.
2) Time to working app, stack used, code quality, manual fixes needed, and how well it fits your workflow.

<!-- STEP:09 -->
## Step 09: AI App Builder Architecture

# AI App Builder Architecture

Under the hood, v0 and peers rely on:

- **WebContainers** to run generated code safely.
- **System prompts** that constrain outputs to well-known stacks (Next.js, Tailwind, Prisma).
- **Action tags** (`boltartifact/Boltaction`) to mark file writes, command runs, and previews.

Users keep iterating‚Äîchange the palette, update icons, tweak typography‚Äîand the agent regenerates both frontend and backend. Lovable, Replit Agents, and Base44 follow similar patterns.

**Coding exercise ‚Äì Build a non-JS alternative.** Create the same app in a different language/framework to understand stack diversity:

```bash
# Django version
django-admin startproject notes_project
cd notes_project
python manage.py startapp notes

# Create models in notes/models.py:
```

```python
from django.db import models

class Note(models.Model):
    title = models.CharField(max_length=200)
    content = models.TextField()
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

    def __str__(self):
        return self.title
```

```bash
# Create views in notes/views.py:
```

```python
from django.shortcuts import render, get_object_or_404, redirect
from .models import Note
from .forms import NoteForm

def note_list(request):
    notes = Note.objects.all().order_by('-created_at')
    return render(request, 'notes/list.html', {'notes': notes})

def note_create(request):
    if request.method == 'POST':
        form = NoteForm(request.POST)
        if form.is_valid():
            form.save()
            return redirect('note_list')
    else:
        form = NoteForm()
    return render(request, 'notes/form.html', {'form': form})
```

```bash
# Setup and run:
python manage.py makemigrations
python manage.py migrate
python manage.py runserver
```

Document in `django-version/README.md`:

```markdown
# Django Notes App

## Setup
```bash
pip install django
python manage.py migrate
python manage.py runserver
```

## Features
- List all notes
- Create new note
- Edit note
- Delete note
- Django admin interface

## Project Structure
```
notes_project/
  notes/
    models.py      # Database models
    views.py       # View logic
    urls.py        # URL routing
    templates/     # HTML templates
  settings.py      # Project configuration
```
```

## Step Quiz
**Questions**
1) Why do app builders constrain agents to established frameworks?
2) What's the benefit of implementing the same app in multiple languages?
**Answers**
1) Familiar stacks keep outputs maintainable and auditable, making it easier for humans to take over when necessary.
2) It helps you understand framework tradeoffs, learn different patterns, and choose the best tool for specific requirements.

<!-- STEP:10 -->
## Step 10: Limitations and Responsible Use

# Limitations and Responsible Use

All this power comes with caveats:

- When AI scaffolding breaks, you‚Äôre back to traditional debugging.
- Prompts are requests, not contracts‚Äîexpect occasional drift.
- Generated apps can look same-y if you don‚Äôt customize.
- Complex, highly regulated systems still exceed current agent capacity.

Use these tools to accelerate prototypes and scaffolding, then hand off to experienced engineers for testing, security, and hardening.

**Coding exercise ‚Äì Security hardening checklist.** After AI generates an app, apply security best practices. Create `SECURITY_HARDENING.md`:

```markdown
# Security Hardening Checklist

## Input Validation
- [ ] All user inputs are validated
- [ ] SQL injection prevention (parameterized queries)
- [ ] XSS prevention (escape outputs)
- [ ] File upload restrictions (type, size)

## Authentication & Authorization
- [ ] Passwords are hashed (bcrypt/argon2)
- [ ] Session tokens are secure (httpOnly, secure flags)
- [ ] CSRF protection enabled
- [ ] Rate limiting on auth endpoints

## Configuration
- [ ] DEBUG=False in production
- [ ] Secret keys in environment variables
- [ ] HTTPS enforced
- [ ] CORS configured properly

## Dependencies
- [ ] All packages up to date
- [ ] No known CVEs in dependencies
- [ ] Minimal dependency footprint

## Changes Made

### Django Version
1. **Added input validation**
   ```python
   # Before (AI generated)
   def create_note(request):
       title = request.POST['title']
       
   # After (hardened)
   def create_note(request):
       title = request.POST.get('title', '').strip()
       if not title or len(title) > 200:
           return HttpResponseBadRequest("Invalid title")
   ```

2. **Fixed CSRF protection**
   - Ensured {% csrf_token %} in all forms
   - Added @csrf_protect decorators

3. **Updated dependencies**
   - Django 4.2 ‚Üí 5.0 (security patches)
   - Added django-ratelimit

### Reasoning
AI-generated code prioritizes functionality over security.
Production apps need explicit hardening against common attacks.
```

Apply these checks to every AI-generated application before deploying.

## Step Quiz
**Questions**
1) What should teams be ready to do when AI-built apps break?
2) What security concerns should you always check after AI generates code?
**Answers**
1) Drop back into traditional debugging and engineering workflows, because the agent's generated scaffolding still needs human maintenance.
2) Input validation, SQL injection, XSS, authentication security, secret management, and dependency vulnerabilities.
