---
lesson_id: "02-ai-ide-agent"
lesson_title: "AI IDEs and Agent Management"
source_dir: "mswd/02-ai-ide-agent"
step_count: 10
---

<!-- STEP:01 -->
## Step 01: Evolution of the AI IDE

# Evolution of the AI IDE

Let’s set the stage. IDEs have always evolved alongside developer workflows:

- **Turbo Pascal (1983)** bundled editor, compiler, linker into one Pascal cockpit.
- **Visual Studio (1997)** popularized advanced debugging plus the first IntelliSense.
- **IntelliJ IDEA (2001)** doubled down on contextual navigation and refactors.
- **VS Code (2015)** brought a lightweight, extensible editor and spawned the Language Server Protocol.
- **Cursor (2023)** leans fully into AI-in-the-loop for every surface.

Notice the pattern: consolidation vs customization in a constant tug-of-war. AI pushes the pendulum again by embedding autonomous help directly inside the workspace you already use.

## Step Quiz
**Questions**
1) What historical pattern connects Turbo Pascal, Visual Studio, IntelliJ, VS Code, and Cursor?
**Answers**
1) Each wave consolidates more capability into the IDE while still leaving room for power users to customize the workflow.

<!-- STEP:02 -->
## Step 02: Modes of an AI IDE

# Modes of an AI IDE

When you open an AI IDE, choose the interaction mode that matches your task:

- **Inline/tab-complete** for quick single-line edits.
- **Command palette chats (Cmd-K)** for questions or small refactors.
- **Multi-file agents (Cmd-L)** for cross-file reasoning, MCP tool use, or background analysis.
- **Ambient agents** (Bugbot, memory bots) for true AI-native experiences that run alongside you.

Treat these like gears on a bike—you shift up or down depending on terrain. Mastery is knowing when a job needs inline assistance versus a full-blown agent.

## Step Quiz
**Questions**
1) Why do AI IDEs offer multiple invocation modes instead of a single chat box?
**Answers**
1) Because different tasks demand different context sizes and autonomy levels, from quick inline edits to multi-file refactors or background agents.

<!-- STEP:03 -->
## Step 03: How AI IDEs Work Under the Hood

# How AI IDEs Work Under the Hood

Peek behind the curtain:

1. **Tab-complete** encrypts a small context window around your cursor, sends it to an infilling model, and streams the suggestion back.
2. **Chat modes** chunk your repo into embeddings, store them in a semantic index (frequently updated via Merkle-tree diffs), and retrieve relevant snippets whenever you ask a question.
3. Files listed in `.cursorignore` or similar configs stay out of the index, which matters for privacy and accuracy.

Understanding this pipeline explains latency spikes, stale references, or why a file isn’t showing up in answers.

## Step Quiz
**Questions**
1) What role do embeddings play in AI IDE chat experiences?
**Answers**
1) They encode code snippets into vectors so the IDE can retrieve relevant context and feed it into the LLM when you ask questions.

<!-- STEP:04 -->
## Step 04: Writing Specs for Complex Tasks

# Writing Specs for Complex Tasks

Simple edits? Fire-and-forget prompts are fine. Complex features? You need a spec. Think of it as a mini PRD:

- **Goal**: why this change matters
- **Definitions & background**: what the agent must know
- **Plan**: high-level steps or components
- **Relevant files**: where to look
- **Tests**: commands to prove success
- **Edge cases/out-of-scope**: guardrails

Feed that doc to the agent and you’ll get deterministic output instead of chaos. Iterate on the template until it mirrors your team’s style.

**Coding exercise – Week 2 LLM extractor.** `week2/app/services/extract.py` contains a heuristic `extract_action_items()` function. Draft a spec in `week2/writeup.md` (or a separate `SPEC.md`) describing how you’ll implement `extract_action_items_llm()` with Ollama: inputs, JSON contract, error cases, and tests (`pytest week2/tests/test_extract.py`). Once the spec is written, add the new function:

```python
def extract_action_items_llm(text: str) -> list[str]:
    response = chat(
        model="llama3.1:8b",
        messages=[
            {"role": "system", "content": "You extract todos as JSON lists."},
            {"role": "user", "content": text},
        ],
        format={"type": "json", "schema": {"items": {"type": "string"}}},
    )
    return json.loads(response.message.content)["items"]
```

Update the FastAPI endpoint to call the LLM version and record the steps you took in the write-up.

## Step Quiz
**Questions**
1) What sections belong in a spec you hand to an AI IDE for complex work?
2) Which file do you extend to add the LLM-powered action item extractor?
**Answers**
1) Goal, definitions, plan, relevant files, tests, edge cases, out-of-scope notes, and potential extensions.
2) `week2/app/services/extract.py`.

<!-- STEP:05 -->
## Step 05: Organizing Repos for Agents

# Organizing Repos for Agents

Agents are your most forgetful teammates—they only know what you tell them. Help them succeed by:

- Maintaining `CLAUDE.md`, `.cursorrules`, or `AGENTS.md` with build steps, tests, naming conventions, and rules of thumb.
- Keeping directory structures predictable and lint clean.
- Documenting DB access patterns or API contracts in the repo.

If you can onboard a new engineer with your docs, you can onboard an agent. If not, start writing.

**Coding exercise – Guidance files.** In `week4/`, create a root-level `CLAUDE.md` that documents how to run `make run`, where routers live (`backend/app/routers`), safe commands, and repo conventions. Add nested `week4/backend/CLAUDE.md` if you want API-specific hints. Claude Code automatically loads these files, so your instructions become part of every conversation.

## Step Quiz
**Questions**
1) What is the purpose of files like `AGENTS.md` or `CLAUDE.md`?
2) Where should you document repo-specific run/lint/test commands for the week4 starter app?
**Answers**
1) They provide agent-specific onboarding instructions—build commands, code style, and project rules that models automatically pull into context.
2) In `week4/CLAUDE.md` (and optional subfolder variants).

<!-- STEP:06 -->
## Step 06: Sync vs Async Coding Tools

# Sync vs Async Coding Tools

Silas Alberti splits tooling across two axes: local vs cloud, synchronous vs asynchronous. Here’s how that feels in practice:

- **Local + sync** (Copilot) → inline boosts (~10%).
- **Local + async-lite** (AI IDE multi-file) → single-player task completion (~20%).
- **Cloud + async** (Devin) → true workflow delegation (6–12× leverage) as agents run for minutes or hours in isolated environments.

Avoid the “semi-async” trap where you wait idly for a response. Either keep tasks short enough to stay in flow or fully hand them off and context-switch.

**Coding exercise – Warp multi-agent setup.** During Week 5 you’ll coordinate multiple Warp tabs. Clone the starter app under `week5/`, run `make run`, then open two Warp tabs:

1. Tab A: `/Users/.../week5` running `make test`.
2. Tab B: `/Users/.../week5` executing a Warp Drive prompt that edits the frontend.

Record how long you wait in each tab. If a task takes longer than a few minutes, move to another tab/task so you stay in true async mode.

## Step Quiz
**Questions**
1) Why is “semi-async” work considered a trap?
2) Which assignment explicitly asks you to run concurrent agents in separate Warp tabs?
**Answers**
1) It’s too long to stay in flow yet too short to switch tasks, so you incur context-switching pain without the benefits of true delegation.
2) Week 5 (Warp multi-agent workflows inside `week5/docs/TASKS.md`).

<!-- STEP:07 -->
## Step 07: Managing Hand-Offs

# Managing Hand-Offs

Think like a tech lead running a pod: plan → build → review.

1. **Plan (human)**: gather context via DeepWiki/Codemaps, clarify scope, document acceptance criteria.
2. **Build (agent)**: delegate the task so Devin (or similar) executes, tests, and iterates asynchronously.
3. **Review (human)**: evaluate the PR, request tweaks, and merge.

Keep a queue so agents are always busy, and treat them like junior teammates with explicit SLOs and check-ins.

**Coding exercise – Track prompts in writeups.** For Week 2 and Week 5 assignments, every task you hand to Cursor or Warp must be documented in `writeup.md`. Create a section per feature:

```markdown
## TODO 2 — Unit tests
- Prompt: "Add pytest cases covering bullet lists and blank inputs."
- Agent output summary: ...
- Manual tweaks: ...
```

This forces you to reflect on the hand-off each time you let an agent take over.

## Step Quiz
**Questions**
1) Where should humans stay in the loop when collaborating with async agents?
2) Which file captures the prompts and outputs you use while delegating in Weeks 2 and 5?
**Answers**
1) During planning (to clarify scope) and final review (to ensure quality), while agents handle most of the execution in between.
2) `writeup.md` inside the relevant week folder.

<!-- STEP:08 -->
## Step 08: Becoming an Agent Manager

# Becoming an Agent Manager

Boris Cherny calls the new role “agent manager.” Your career arc now looks like:

1. Solo developer
2. Tech lead managing humans
3. Tech lead assisted by AI
4. Single developer managing many AI agents

Every engineer effectively runs a team of personas: frontend specialist, backend specialist, PR reviewer, QA bot. You supply requirements, break down work, spot-check quality, and ensure CI stays green.

**Coding exercise – Define subagent roles.** In Week 4 you can create role-specific files under `.claude/subagents/` (or document them in `writeup.md`). For example:

```markdown
<!-- .claude/subagents/tester.md -->
You are TestAgent. Responsibilities:
- Update files in backend/tests before implementation changes.
- Run `make test` after edits and paste results.
- Never modify frontend files.
```

Then spin up a CodeAgent with complementary instructions. Practice alternating between them using `/agent TestAgent` inside Claude Code.

## Step Quiz
**Questions**
1) What new responsibility does an “agent manager” engineer take on?
2) Where should you describe the personas you hand off to when building Claude Code automations?
**Answers**
1) Directing multiple specialized agents (or humans) by supplying requirements, specs, and oversight instead of only writing code personally.
2) In dedicated SubAgent prompt files (e.g., `.claude/subagents/*.md`) referenced in Week 4.

<!-- STEP:09 -->
## Step 09: Hooks, Commands, and Subagents

# Hooks, Commands, and Subagents

To orchestrate your agent army, lean on three primitives:

- **Hooks**: deterministic scripts tied to events (`PreToolUse`, `PostToolUse`, `UserPromptSubmit`) for logging, guardrails, or context compaction.
- **Commands**: reusable prompt files for common tasks (`run-tests`, `ship-it`, `lint-and-fix`).
- **Subagents**: dedicated personas with unique system prompts, tools, and memory windows (frontend expert, security reviewer, doc writer).

Mix and match these surfaces to tailor behavior without rewriting the core runtime.

**Coding exercise – Custom slash command.** Inside `week4/.claude/commands/`, add a file such as `tests.md`:

```markdown
# /tests
You run backend tests with coverage.

## Arguments
- path (optional): Limit pytest run.

## Steps
1. Run `poetry run pytest {{path or 'backend/tests'}}`.
2. If tests fail, paste the failure summary and suggest fixes.
3. If tests pass, run `coverage run -m pytest` and report coverage %.
```

Restart Claude Code so `/tests` appears in the slash command picker.

## Step Quiz
**Questions**
1) When would you reach for a subagent instead of a command?
2) Where do you store reusable slash commands for the Week 4 automation assignment?
**Answers**
1) When you need a distinct persona with its own tools and long-running context, not just a single reusable prompt.
2) `.claude/commands/*.md` inside the repository (e.g., `week4/.claude/commands/tests.md`).

<!-- STEP:10 -->
## Step 10: Guardrails and Backstops

# Guardrails and Backstops

Delegation without guardrails is a recipe for outages. Keep these safety nets:

- Comprehensive tests and CI
- Audit logs tagging agent-generated diffs
- Frequent commits/checkpoints
- Tiered model usage (expensive planners vs fast executors)
- Access controls around sensitive directories or commands

Treat agents like high-speed interns—they’re powerful but only when paired with instrumentation and oversight.

**Coding exercise – Enforce guardrails.** Leverage the existing Makefile targets under `week4/` and `week5/`. After each agent session, run:

```bash
make format
make lint
make test
```

If the agent broke something, capture the failure logs in `writeup.md` and guide it (or fix manually) before pushing.

## Step Quiz
**Questions**
1) Name two guardrails you should keep in place when delegating to coding agents.
2) Which commands should you automate at the end of every Claude/Warp session to catch regressions in the starter apps?
**Answers**
1) Maintain automated tests/CI and audit every agent-generated diff (with commits or labels) before merging.
2) `make format`, `make lint`, and `make test` (plus any additional project-specific checks).
