---
lesson_id: "02-ai-ide-agent"
lesson_title: "AI IDEs and Agent Management"
source_dir: "mswd/02-ai-ide-agent"
step_count: 10
---

<!-- STEP:01 -->
## Step 01: Evolution of the AI IDE

# Evolution of the AI IDE

Let‚Äôs set the stage. IDEs have always evolved alongside developer workflows:

- **Turbo Pascal (1983)** bundled editor, compiler, linker into one Pascal cockpit.
- **Visual Studio (1997)** popularized advanced debugging plus the first IntelliSense.
- **IntelliJ IDEA (2001)** doubled down on contextual navigation and refactors.
- **VS Code (2015)** brought a lightweight, extensible editor and spawned the Language Server Protocol.
- **Cursor (2023)** leans fully into AI-in-the-loop for every surface.

Notice the pattern: consolidation vs customization in a constant tug-of-war. AI pushes the pendulum again by embedding autonomous help directly inside the workspace you already use.

## Step Quiz
**Questions**
1) What historical pattern connects Turbo Pascal, Visual Studio, IntelliJ, VS Code, and Cursor?
**Answers**
1) Each wave consolidates more capability into the IDE while still leaving room for power users to customize the workflow.

<!-- STEP:02 -->
## Step 02: Modes of an AI IDE

# Modes of an AI IDE

When you open an AI IDE, choose the interaction mode that matches your task:

- **Inline/tab-complete** for quick single-line edits.
- **Command palette chats (Cmd-K)** for questions or small refactors.
- **Multi-file agents (Cmd-L)** for cross-file reasoning, MCP tool use, or background analysis.
- **Ambient agents** (Bugbot, memory bots) for true AI-native experiences that run alongside you.

Treat these like gears on a bike‚Äîyou shift up or down depending on terrain. Mastery is knowing when a job needs inline assistance versus a full-blown agent.

## Step Quiz
**Questions**
1) Why do AI IDEs offer multiple invocation modes instead of a single chat box?
**Answers**
1) Because different tasks demand different context sizes and autonomy levels, from quick inline edits to multi-file refactors or background agents.

<!-- STEP:03 -->
## Step 03: How AI IDEs Work Under the Hood

# How AI IDEs Work Under the Hood

Peek behind the curtain:

1. **Tab-complete** encrypts a small context window around your cursor, sends it to an infilling model, and streams the suggestion back.
2. **Chat modes** chunk your repo into embeddings, store them in a semantic index (frequently updated via Merkle-tree diffs), and retrieve relevant snippets whenever you ask a question.
3. Files listed in `.cursorignore` or similar configs stay out of the index, which matters for privacy and accuracy.

Understanding this pipeline explains latency spikes, stale references, or why a file isn‚Äôt showing up in answers.

## Step Quiz
**Questions**
1) What role do embeddings play in AI IDE chat experiences?
**Answers**
1) They encode code snippets into vectors so the IDE can retrieve relevant context and feed it into the LLM when you ask questions.

<!-- STEP:04 -->
## Step 04: Writing Specs for Complex Tasks

# Writing Specs for Complex Tasks

Simple edits? Fire-and-forget prompts are fine. Complex features? You need a spec. Think of it as a mini PRD:

- **Goal**: why this change matters
- **Definitions & background**: what the agent must know
- **Plan**: high-level steps or components
- **Relevant files**: where to look
- **Tests**: commands to prove success
- **Edge cases/out-of-scope**: guardrails

Feed that doc to the agent and you‚Äôll get deterministic output instead of chaos. Iterate on the template until it mirrors your team‚Äôs style.

**Coding exercise ‚Äì LLM-powered text extraction.** Let's build a complete action item extractor. First, create a spec document `SPEC.md`:

```markdown
# LLM Action Item Extractor Spec

## Goal
Replace heuristic text extraction with LLM-powered parsing to improve accuracy.

## Implementation Plan
1. Create `extract_action_items_llm()` function
2. Use Ollama with llama3.1:8b model
3. Return JSON list of action items
4. Handle error cases (network, parsing, empty responses)

## JSON Contract
Input: Plain text string
Output: `{"items": ["todo 1", "todo 2", ...]}`

## Error Cases
- Network timeout ‚Üí return empty list
- Invalid JSON ‚Üí log error, return empty list
- Empty text ‚Üí return empty list
```

Now create `extract_service.py` with the implementation:

```python
import json
from typing import List
from ollama import chat

def extract_action_items_llm(text: str) -> List[str]:
    """Extract action items from text using LLM.
    
    Args:
        text: Input text containing potential action items
        
    Returns:
        List of extracted action item strings
    """
    if not text or not text.strip():
        return []
    
    try:
        response = chat(
            model="llama3.1:8b",
            messages=[
                {"role": "system", "content": "You extract todos as JSON lists. Only return JSON."},
                {"role": "user", "content": text},
            ],
            format={"type": "json", "schema": {"items": {"type": "array", "items": {"type": "string"}}}},
        )
        result = json.loads(response.message.content)
        return result.get("items", [])
    except Exception as e:
        print(f"Error extracting action items: {e}")
        return []

# Example usage
if __name__ == "__main__":
    sample_text = """
    Meeting notes:
    - TODO: Review the Q4 budget
    - Need to schedule team sync
    - Action item: Update the documentation
    """
    
    items = extract_action_items_llm(sample_text)
    print("Extracted action items:")
    for item in items:
        print(f"  - {item}")
```

Test the implementation with `python extract_service.py` and document your results.

## Step Quiz
**Questions**
1) What sections belong in a spec you hand to an AI IDE for complex work?
2) Why is error handling important in LLM-powered services?
**Answers**
1) Goal, definitions, plan, relevant files, tests, edge cases, out-of-scope notes, and potential extensions.
2) LLMs can fail due to network issues, rate limits, or invalid outputs, so proper error handling prevents service crashes.

<!-- STEP:05 -->
## Step 05: Organizing Repos for Agents

# Organizing Repos for Agents

Agents are your most forgetful teammates‚Äîthey only know what you tell them. Help them succeed by:

- Maintaining `CLAUDE.md`, `.cursorrules`, or `AGENTS.md` with build steps, tests, naming conventions, and rules of thumb.
- Keeping directory structures predictable and lint clean.
- Documenting DB access patterns or API contracts in the repo.

If you can onboard a new engineer with your docs, you can onboard an agent. If not, start writing.

**Coding exercise ‚Äì Agent guidance file.** Create a `CLAUDE.md` file to help AI agents understand your project:

```markdown
# Project Guide for AI Agents

## Quick Start
```bash
# Run the application
make run

# Run tests
make test

# Format code
make format
```

## Project Structure
```
backend/
  app/
    routers/     # API route handlers
    services/    # Business logic
    models.py    # Database models
  tests/         # Test files
```

## Coding Conventions
- Use type hints for all function parameters
- Write docstrings for public functions
- Keep router handlers thin, move logic to services
- All database operations must use parameterized queries

## Safe Commands
‚úÖ Allowed:
- pytest, make test
- make format, make lint
- uvicorn with --reload

‚õî Forbidden:
- rm -rf
- curl | sh
- Direct database modifications

## Common Tasks
### Adding a new endpoint
1. Add route in `backend/app/routers/`
2. Add service logic in `backend/app/services/`
3. Write tests in `backend/tests/`
4. Run `make test` to verify
```

This file serves as automatic onboarding for any AI agent working on your codebase.

## Step Quiz
**Questions**
1) What is the purpose of files like `AGENTS.md` or `CLAUDE.md`?
2) What type of commands should be marked as forbidden?
**Answers**
1) They provide agent-specific onboarding instructions‚Äîbuild commands, code style, and project rules that models automatically pull into context.
2) Destructive commands like `rm -rf`, arbitrary script execution like `curl | sh`, or any commands that could damage data or security.

<!-- STEP:06 -->
## Step 06: Sync vs Async Coding Tools

# Sync vs Async Coding Tools

Silas Alberti splits tooling across two axes: local vs cloud, synchronous vs asynchronous. Here‚Äôs how that feels in practice:

- **Local + sync** (Copilot) ‚Üí inline boosts (~10%).
- **Local + async-lite** (AI IDE multi-file) ‚Üí single-player task completion (~20%).
- **Cloud + async** (Devin) ‚Üí true workflow delegation (6‚Äì12√ó leverage) as agents run for minutes or hours in isolated environments.

Avoid the ‚Äúsemi-async‚Äù trap where you wait idly for a response. Either keep tasks short enough to stay in flow or fully hand them off and context-switch.

**Coding exercise ‚Äì Async workflow simulation.** Practice coordinating multiple tasks asynchronously. Set up a simple project and try working on two tasks simultaneously:

1. **Terminal 1**: Start a development server or run continuous tests
   ```bash
   # Option A: Run tests in watch mode
   pytest --watch
   
   # Option B: Run development server
   python manage.py runserver
   ```

2. **Terminal 2**: Work on a separate feature using an AI IDE agent
   - Ask the agent to implement a new API endpoint
   - Or refactor existing code
   - Or write documentation

Track your workflow:
- Record start time for each task
- Note when you switch contexts
- Measure total time saved by working asynchronously

**Goal**: If a task takes longer than 2-3 minutes, you should be able to switch to another task and come back later. This is true async work.

## Step Quiz
**Questions**
1) Why is "semi-async" work considered a trap?
2) What's a good threshold for when to switch tasks?
**Answers**
1) It's too long to stay in flow yet too short to switch tasks, so you incur context-switching pain without the benefits of true delegation.
2) If a task takes longer than 2-3 minutes to complete, it's a good candidate for async work where you can context-switch to another task.

<!-- STEP:07 -->
## Step 07: Managing Hand-Offs

# Managing Hand-Offs

Think like a tech lead running a pod: plan ‚Üí build ‚Üí review.

1. **Plan (human)**: gather context via DeepWiki/Codemaps, clarify scope, document acceptance criteria.
2. **Build (agent)**: delegate the task so Devin (or similar) executes, tests, and iterates asynchronously.
3. **Review (human)**: evaluate the PR, request tweaks, and merge.

Keep a queue so agents are always busy, and treat them like junior teammates with explicit SLOs and check-ins.

**Coding exercise ‚Äì Document your AI workflow.** Create a `AI_WORKFLOW.md` file to track how you use AI agents:

```markdown
# AI Agent Workflow Documentation

## Feature 1: User Authentication

### Task Description
Implement JWT-based authentication for the API

### Prompt Given to Agent
```
Add JWT authentication to the FastAPI application:
- Create login endpoint that returns JWT token
- Add middleware to verify tokens
- Protect existing endpoints
- Write tests for auth flow
```

### Agent Output Summary
- Generated `auth.py` with JWT utilities
- Added login endpoint in `routers/auth.py`
- Created auth middleware
- Generated 5 test cases

### Manual Tweaks Required
- Fixed token expiration logic (was set to 1 minute, changed to 24 hours)
- Added error handling for expired tokens
- Updated test assertions to match actual response format

### Lessons Learned
- Be specific about token expiration time
- Agent sometimes misses edge cases in error handling
- Test generation is excellent but needs human verification

---

## Feature 2: Data Export

[Document next feature...]
```

This practice helps you learn what prompts work well and where agents need guidance.

## Step Quiz
**Questions**
1) Where should humans stay in the loop when collaborating with async agents?
2) What should you document after each agent interaction?
**Answers**
1) During planning (to clarify scope) and final review (to ensure quality), while agents handle most of the execution in between.
2) The original prompt, agent output summary, manual tweaks required, and lessons learned for future prompts.

<!-- STEP:08 -->
## Step 08: Becoming an Agent Manager

# Becoming an Agent Manager

Boris Cherny calls the new role ‚Äúagent manager.‚Äù Your career arc now looks like:

1. Solo developer
2. Tech lead managing humans
3. Tech lead assisted by AI
4. Single developer managing many AI agents

Every engineer effectively runs a team of personas: frontend specialist, backend specialist, PR reviewer, QA bot. You supply requirements, break down work, spot-check quality, and ensure CI stays green.

**Coding exercise ‚Äì Define specialized agent roles.** Create role-specific prompt files for different tasks. Create a `.agents/` directory with specialized instructions:

```markdown
<!-- .agents/tester.md -->
# Test Agent Role

You are TestAgent, specialized in testing and quality assurance.

## Responsibilities
- Write comprehensive test cases using pytest
- Update test files BEFORE making implementation changes (TDD)
- Run `pytest` after edits and report results
- Focus only on backend/tests directory
- Never modify frontend or production code

## Test Guidelines
- Use descriptive test names (test_user_can_login_with_valid_credentials)
- Cover happy path and error cases
- Mock external dependencies
- Aim for 80%+ code coverage

## Example Workflow
1. Read the specification
2. Write failing tests first
3. Report test results
4. Wait for implementation
5. Verify tests pass
```

```markdown
<!-- .agents/coder.md -->
# Code Agent Role

You are CodeAgent, specialized in writing clean production code.

## Responsibilities
- Implement features to make existing tests pass
- Follow PEP 8 style guidelines
- Write type hints for all functions
- Add docstrings to public APIs
- Focus only on implementation files (not tests)

## Code Guidelines
- Keep functions small and focused
- Use meaningful variable names
- Handle errors explicitly
- Never commit commented-out code

## Example Workflow
1. Review failing tests
2. Implement minimal code to pass tests
3. Refactor for clarity
4. Run linter
5. Report completion
```

Use these prompts with your AI IDE to maintain clear separation of concerns.

## Step Quiz
**Questions**
1) What new responsibility does an "agent manager" engineer take on?
2) Why create separate agent personas?
**Answers**
1) Directing multiple specialized agents (or humans) by supplying requirements, specs, and oversight instead of only writing code personally.
2) Different tasks require different mindsets and constraints - a testing agent should focus on edge cases while a coding agent focuses on implementation, preventing mixed concerns.

<!-- STEP:09 -->
## Step 09: Hooks, Commands, and Subagents

# Hooks, Commands, and Subagents

To orchestrate your agent army, lean on three primitives:

- **Hooks**: deterministic scripts tied to events (`PreToolUse`, `PostToolUse`, `UserPromptSubmit`) for logging, guardrails, or context compaction.
- **Commands**: reusable prompt files for common tasks (`run-tests`, `ship-it`, `lint-and-fix`).
- **Subagents**: dedicated personas with unique system prompts, tools, and memory windows (frontend expert, security reviewer, doc writer).

Mix and match these surfaces to tailor behavior without rewriting the core runtime.

**Coding exercise ‚Äì Custom command templates.** Create reusable command templates for common tasks. Create a `.commands/` directory:

```markdown
<!-- .commands/test.md -->
# Test Command

Run backend tests with coverage reporting.

## Usage
/test [optional-path]

## Steps
1. Run pytest on specified path or default to 'tests/'
   ```bash
   pytest {{path or 'tests/'}} -v --cov
   ```

2. If tests fail:
   - Paste the failure summary
   - Analyze the error
   - Suggest specific fixes

3. If tests pass:
   - Run coverage report
   - Identify untested code
   - Suggest additional test cases if coverage < 80%

## Output Format
```
‚úÖ Tests: X passed, Y failed
üìä Coverage: Z%
üí° Suggestions: [list any recommendations]
```
```

```markdown
<!-- .commands/review.md -->
# Code Review Command

Perform a thorough code review of recent changes.

## Usage
/review [file-path]

## Checklist
- [ ] Type hints on all function parameters
- [ ] Docstrings for public functions
- [ ] Error handling for external calls
- [ ] No hard-coded credentials or secrets
- [ ] Tests cover happy path and edge cases
- [ ] Code follows project style guide

## Output Format
Provide:
1. What works well
2. Specific issues with line numbers
3. Suggested improvements
4. Security considerations
```

These templates make common workflows consistent and reusable.

## Step Quiz
**Questions**
1) When would you reach for a subagent instead of a command?
2) What's the benefit of command templates?
**Answers**
1) When you need a distinct persona with its own tools and long-running context, not just a single reusable prompt.
2) They standardize common workflows, ensure consistent quality checks, and can be easily shared across team members.

<!-- STEP:10 -->
## Step 10: Guardrails and Backstops

# Guardrails and Backstops

Delegation without guardrails is a recipe for outages. Keep these safety nets:

- Comprehensive tests and CI
- Audit logs tagging agent-generated diffs
- Frequent commits/checkpoints
- Tiered model usage (expensive planners vs fast executors)
- Access controls around sensitive directories or commands

Treat agents like high-speed interns‚Äîthey‚Äôre powerful but only when paired with instrumentation and oversight.

**Coding exercise ‚Äì Create a guardrails checklist.** Set up automated quality checks that run after every AI agent session. Create a `Makefile`:

```makefile
.PHONY: check format lint test all

# Run all quality checks
all: format lint test
	@echo "‚úÖ All quality checks passed!"

# Format code automatically
format:
	@echo "üîß Formatting code..."
	black .
	isort .

# Run linters
lint:
	@echo "üîç Running linters..."
	pylint **/*.py
	mypy .
	
# Run tests
test:
	@echo "üß™ Running tests..."
	pytest --cov --cov-report=term-missing

# Pre-commit hook check
pre-commit:
	@echo "üéØ Running pre-commit checks..."
	$(MAKE) format
	$(MAKE) lint
	$(MAKE) test
```

Create a checklist document `.agents/CHECKLIST.md`:

```markdown
# Post-Agent Session Checklist

After every AI agent coding session, complete these steps:

## Automated Checks
- [ ] `make format` - Auto-format code
- [ ] `make lint` - Check for code quality issues
- [ ] `make test` - Verify all tests pass

## Manual Review
- [ ] Read generated code for logic errors
- [ ] Check for hardcoded secrets or credentials
- [ ] Verify error handling is present
- [ ] Confirm changes match the specification
- [ ] Review git diff before committing

## Documentation
- [ ] Update README if public API changed
- [ ] Add/update docstrings for new functions
- [ ] Document any manual fixes required
- [ ] Note lessons learned for future prompts

## If Anything Fails
1. Capture the error message
2. Document what broke
3. Fix manually or guide agent to fix
4. Update prompts to avoid issue in future
```

## Step Quiz
**Questions**
1) Name two guardrails you should keep in place when delegating to coding agents.
2) Why is a checklist important when using AI agents?
**Answers**
1) Maintain automated tests/CI and audit every agent-generated diff (with commits or labels) before merging.
2) Checklists ensure consistent quality standards, catch common agent mistakes, and help build better prompts over time by documenting failure patterns.
