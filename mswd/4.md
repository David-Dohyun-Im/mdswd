---
lesson_id: "04-quality-operations"
lesson_title: "AI Quality, Security, and Operations"
source_dir: "mswd/04-quality-operations"
step_count: 10
---

<!-- STEP:01 -->
## Step 01: Why Guardrails Matter More Than Ever

# Why Guardrails Matter More Than Ever

Kick off by grounding yourself in reality: bugs destroy trust and burn cash. Isaac Evans from Semgrep points out that once LLMs write most of your code, you must assume higher defect rates. Classic threats (SQL injection, XSS, broken auth, IDOR, config mistakes) now have more surfaces because agents call tools autonomously. The way forward is not to slow down but to pair AI velocity with disciplined guardrails.

**Coding exercise – Run Semgrep.** From the repo root, scan the Week 6 project:

```bash
semgrep ci --subdir week6
```

Paste the resulting summary into `week6/writeup.md`, noting the mix of SAST, secrets, and dependency findings before you fix anything.

## Step Quiz
**Questions**
1) Why does increased AI-generated code heighten the need for guardrails?
2) Which command triggers the full Semgrep bundle on Week 6?
**Answers**
1) Because more autogenerated changes raise the chance of regressions or vulnerabilities unless you enforce strong testing and security practices.
2) `semgrep ci --subdir week6`.

<!-- STEP:02 -->
## Step 02: Static Analysis (SAST)

# Static Analysis (SAST)

Static Application Security Testing is your first safety net. Run Semgrep, Bandit, or ESLint rules on source/binaries **before** the code runs to catch SQL injection, command injection, or XSS patterns. Findings are cheap to fix early, and you can wire these scans into your agent workflows so every diff—human or AI—gets vetted automatically. Tune rules carefully to avoid noise, but don’t skip this stage.

**Coding exercise – Patch a finding.** Pick one `semgrep ci` finding in `week6/backend/` (for example, an unsafe SQL string) and fix it. If the report flags string interpolation inside `repository.py`, refactor to parameterized SQL:

```python
result = await db.execute(
    text("SELECT * FROM notes WHERE title = :title"),
    {"title": title},
)
```

Re-run Semgrep and note the resolved rule in `week6/writeup.md`.

## Step Quiz
**Questions**
1) What types of vulnerabilities does SAST excel at catching?
2) Where do you record the Semgrep fix you implemented?
**Answers**
1) Static code flaws such as SQL injection, command injection, and cross-site scripting patterns.
2) In `week6/writeup.md`, alongside the issue summary.

<!-- STEP:03 -->
## Step 03: Dynamic Analysis (DAST)

# Dynamic Analysis (DAST)

DAST complements SAST by treating your app like a black box. Fuzz inputs, tamper with tokens, brute force rate limits, or inspect headers while the service is running. Because you’re observing real behavior, false positives drop, and you can run these checks continuously in staging or production mirrors. When the AI forgets a validation step, DAST is often the mechanism that catches it before customers do.

**Coding exercise – Curl the running app.** Start the Week 6 FastAPI app:

```bash
uvicorn week6.backend.app.main:app --reload
```

Then send malicious payloads via `curl` or a REST client to confirm whether SAST findings reproduce at runtime. Document the request/response in `week6/writeup.md`.

## Step Quiz
**Questions**
1) Why does DAST tend to report fewer false positives than SAST?
2) Which command spins up the Week 6 FastAPI server for manual probing?
**Answers**
1) It observes actual runtime behavior, so findings usually correspond to exploitable flaws rather than theoretical patterns.
2) `uvicorn week6.backend.app.main:app --reload`.

<!-- STEP:04 -->
## Step 04: Software Composition Analysis (SCA)

# Software Composition Analysis (SCA)

SCA tracks the third-party code you ship. Scan package manifests, IaC files, and container images, resolve transitive dependencies, and match them against CVE databases. AI can introduce new packages without human review, so automated SCA ensures compromised libraries never make it to production unnoticed.

**Coding exercise – Audit dependencies.** Run an SCA tool (e.g., `pip-audit` or `poetry export` + `safety check`) against `week6/requirements.txt`. If a dependency is vulnerable, upgrade it and note the CVE in your write-up.

## Step Quiz
**Questions**
1) What does SCA analyze that SAST/DAST typically do not?
2) Which file lists the dependencies you should audit in Week 6?
**Answers**
1) The open-source components and dependencies your application includes, including transitive packages and container images.
2) `week6/requirements.txt`.

<!-- STEP:05 -->
## Step 05: New AI Attack Vectors

# New AI Attack Vectors

Agents open doors to attacks we rarely faced before:

- **Prompt injection** – hide malicious text in files/HTML so the model disobeys instructions.
- **Tool misuse** – trick the agent into abusing its own integrations.
- **Intent breaking** – manipulate the plan midstream and redirect actions.
- **Identity spoofing** – compromise auth tokens to impersonate agents.
- **Code attacks** – exploit the agent’s ability to run commands and escape sandboxes.

You need training, monitoring, and sandboxing strategies specifically for these vectors.

**Coding exercise – Harden CLAUDE.md.** In `week4/CLAUDE.md`, add guardrails that tell Claude to ignore files containing `<!-- prompt injection -->` comments and forbid running commands like `rm -rf` or `curl | sh` without explicit confirmation. This gives you a written defense-in-depth layer when experimenting with automation.

## Step Quiz
**Questions**
1) Give two examples of attack vectors unique to agentic systems.
2) Which guidance file should you update to steer agents away from prompt injection traps?
**Answers**
1) Prompt injection (hidden instructions) and tool misuse (tricking the agent into abusing an integrated tool).
2) `week4/CLAUDE.md`.

<!-- STEP:06 -->
## Step 06: LLMs for Security and Testing

# LLMs for Security and Testing

AI isn’t just a risk; it’s a force multiplier for AppSec. Use LLMs to triage SAST findings, draft penetration test plans, or generate regression tests automatically. Still, temper expectations: false positives can reach 50–100%, runs can disagree, and context windows lose fidelity over time. Always capture evidence, compact summaries responsibly, and keep humans in the loop for final sign-off.

**Coding exercise – AI triage note.** Take one Semgrep finding and ask Claude/Cursor to summarize the risk plus remediation steps. Paste both the prompt and response into `week6/writeup.md`, making clear where you agree or disagree with the AI’s advice.

## Step Quiz
**Questions**
1) What is a major limitation of today’s AI-driven security analysis?
2) Where should you store the AI-generated triage summary?
**Answers**
1) High false-positive rates and inconsistent reasoning across runs make it risky to trust without human review.
2) In `week6/writeup.md` next to the corresponding finding.

<!-- STEP:07 -->
## Step 07: The Value of Code Review

# The Value of Code Review

Code review remains one of the highest leverage activities you can do. Studies cited by Tomas Reimers show 55–60% error detection rates compared to 25–45% for other testing modes, and defects per 100 lines plummet when review is standard practice. Good reviews:

- Reference specific lines or functions
- Explain why something is risky
- Offer alternatives or questions instead of “looks bad”

AI can draft the code, but humans still own the result when it reaches production.

**Coding exercise – Week 7 PR checklist.** For each task in `week7/docs/TASKS.md`, open a dedicated branch, implement the change, and record manual review notes directly in the PR description or `writeup.md`. Mention the commands you ran (e.g., `pytest`, `npm test`) and highlight any issues you caught that the agent missed.

## Step Quiz
**Questions**
1) Name two qualities of a good code review comment.
2) Which week’s assignment asks you to record manual review notes for each PR?
**Answers**
1) It references concrete code and proposes a rationale or potential resolution rather than vague criticism.
2) Week 7 (Graphite code review).

<!-- STEP:08 -->
## Step 08: AI-Augmented Code Review

# AI-Augmented Code Review

Modern tools (Graphite, Greptile, Coderabbit, Claude Code) augment review by:

- Flagging missing tests or risky patterns
- Summarizing diffs so humans focus on architecture
- Providing consistent enforcement of style guides

They also introduce trade-offs: setup/configuration overhead, false positives, and blind spots in domain-specific logic. Use them as companion reviewers that free humans to focus on nuance, not as automated merge buttons.

**Coding exercise – Archive AI review links.** When you request a Graphite Diamond review for each Week 7 PR, copy the link or screenshots of the AI comments into `week7/writeup.md`. Compare at least one AI suggestion with your manual review to decide whether to accept it.

## Step Quiz
**Questions**
1) What should developers remain cautious about when using AI for code review?
2) Where do you store the AI review links for grading?
**Answers**
1) AI often misses complex business logic or security nuances, so humans must still make the final quality call.
2) In `week7/writeup.md`.

<!-- STEP:09 -->
## Step 09: The Old World of DevOps

# The Old World of DevOps

Shift gears to operations. Classic SRE life revolves around the **four golden signals** (latency, errors, traffic, saturation), on-call rotations, and runbooks that often lag reality. A typical incident playbook:

1. Acknowledge the page
2. Check database and app dashboards
3. Identify recent deploys or config changes
4. Localize the blast radius
5. Apply mitigations
6. Stabilize, monitor, and communicate updates every 10–15 minutes
7. Document root cause and follow-ups

Metrics like MTTR and number of engineers per incident define success.

**Coding exercise – Draft a runbook.** Create `week6/docs/INCIDENT.md` (or similar) containing the steps above tailored to the FastAPI app. Include commands such as `tail -f backend/logs/app.log`, `sqlite3 data/app.db`, and `pytest backend/tests` so you can respond quickly when something breaks.

## Step Quiz
**Questions**
1) What are the “four golden signals” every SRE monitors?
2) Where should you store a custom incident playbook for the Week 6 app?
**Answers**
1) Latency, errors, traffic (requests/second), and saturation (resource usage).
2) In a new doc such as `week6/docs/INCIDENT.md`.

<!-- STEP:10 -->
## Step 10: AI DevOps and the New World

# AI DevOps and the New World

Now watch AI reshape operations. Platforms such as Resolve AI, Datadog Bits AI, and Splunk Observability Assistant build dynamic knowledge graphs across services, correlate traces/logs/metrics, and narrate likely root causes with evidence. You still need to apply mitigations, but diagnosis accelerates dramatically. Key principles:

- Emphasize explainability so humans trust the agent’s suggestions
- Instrument everything; AI can’t reason about signals you never collected
- Recognize limits: heterogeneous stacks and complex incidents still require human judgment

The long-term goal is auto-remediation, but today the win is faster, evidence-backed triage.

**Coding exercise – Capture evidence.** While running Week 6 locally, script a quick telemetry capture (even a shell script) that records `curl -w "%{time_total}"`, `uvicorn` logs, and SQLite stats into `week6/logs/incident.txt`. Upload the artifact when writing your incident summary so AI assistants like Resolve have raw data to ingest later.

## Step Quiz
**Questions**
1) What is the immediate value AI DevOps agents provide today?
2) What artifact should you create when simulating incidents locally?
**Answers**
1) They accelerate root cause analysis by correlating telemetry and presenting evidence-backed narratives, even if humans still apply the fixes.
2) A log/telemetry bundle (e.g., `week6/logs/incident.txt`) that captures the evidence AI agents can analyze.
