Structured Writing for Professionals
English 170 
Stanford University, Fall 2025
Mihail Eric
Welcome to english 170 - structured writing for professionals

The Modern Software Developer
CS146S 
Stanford University, Fall 2025
Mihail Eric
themodernsoftware.dev
Jokes! It’s 8:30 am after all
But as we’ll see this class we will spend a lot of time learning to write well-structured english prose, more than sophisticated fancy code
This is the way that software engineering is evolving as we learn to interact with modern AI systems

For whom is this your first class of the quarter? First class at Stanford? Exciting, you’ll either walk out of this thinking I’m so lucky to be @ Stanford, or when’s the deadline for me to get refunded on my tuition?	


Introduction and How LLMs are Made
themodernsoftware.dev
Today I’ll given an overview for what the structure and topics of the course
Then we’ll spend some time doing a how the sausage is made for LLMs
The most “AI”-ey lecture we’ll have all quarter so stick with me

State of the World: 2025 
themodernsoftware.dev
The generative AI revolution is poised to revolutionize just about every industry out there
New headlines come up every day about the advances that have been made 
Huge capital investment from major tech corporations, it is likely the closest we have been to AGI in human history
In the 13 years I’ve been doing AI, the excitement level for the field is higher than it’s ever been 

Bad News
themodernsoftware.dev
Windsurf team
But for software, there are some hard truths
Not going to mince words, software engineering as an industry is getting shaken - we’re experiencing a huge overhaul for the entire ecosystem
20% drop in enrollment for this major
The only professions with higher unemployment rate: graphic design (hello midjourney/veo3), chemistry (have you ever met a chemist?), anthropology (no one’s paying to find out how humans descended from homo habilis)

Good News
Software developers have the potential to be more productive than they have ever been in history
With AI coding an engineer can pick up tech stacks and tools at an unprecedented pace
You won’t be replaced by AI. You’ll be replaced by a competent engineer who knows how to use AI. 
themodernsoftware.dev
But there is some good news
Students learning CS today have the potential to be an order of magnitude better than I was when I was learning. 
Part of the software engineering identity crisis is about asking ourselves what it really means to be a developer?
If your only value add is knowing how to copy-paste from Stackoverflow then you will be replaced by AI
If you think in terms of systems, understand business context, think through resilient architectures and abstractions, AI will strap a rocket to your back in terms of productivity
I promise I will teach you the state-of-the-art in how to use AI in software development today. This will make you irreplaceable


The Modern Software Developer
themodernsoftware.dev
Which brings us to this course…

This is not the “vibe coding” class
themodernsoftware.dev

I want to set expectation upfront to scare off the GSB students. This is not the vibe coding class. 
Vibe coding has different definitions but the standard one of just YOLO-ing and tab-accepting AI code is not the focus here
I’ll tell you why: vibe coding is just not good enough to truly build good software. Hard to predict but we’re likely 2-10+ years away from that being a viable way to build robust production-level software
Again this is not a class to teach non-technical users how to not have to hire developers.
This is a class for fairly experienced engineers to truly become 10x engineers

 10 weeks in 2 slides
themodernsoftware.dev

The Takeaway
Human-agent engineering 
Focus on the skills that are not yet replaced by AI systems
Business understanding
Become the tech lead
LLMs are only as good as you are
Good context leads to good code
If you can’t understand your codebase, neither will an LLM
themodernsoftware.dev
My alternative to vibe coding: human-agent engineering
These days bad code is both architecturally wrong decisions but also functionally incorrect software (from hallucinating LLMs)

The Takeaway
Read and review a lot of code
Learn to discern good from bad, wrong software
Have good taste
Experiment aggressively
There are no established software patterns yet
Everyone is still figuring it out
This class will introduce many workflows and tools - figure out what works for you
themodernsoftware.dev
Tools will go out of date. I won’t be too prescriptive here. This class will evolve just as the field is evolving

Course Logistics
A bit about me
Stanford undergrad/grad
Head of AI at a stealth startup in the sales space
Built first LLMs at Amazon Alexa
Founded and sold an ML education startup
Founded a YC-backed AI coding company
1 awesome CA
Febie Lin
themodernsoftware.dev

Course Logistics
https://themodernsoftware.dev
Lectures
Mon/Fri 8:30-9:20 am
Deliverables
9 assignments (1x/week) focusing on lecture material practice
https://github.com/mihail911/modern-software-dev-assignments
1 final open-ended project in which you will exercise AI coding principles we cover
Grading
80/15/5 breakdown for project/assignments/participation
Something pretty awesome
Guest lectures from founders leading top AI developer startups today
$100s of millions raised, billions in valuation
Don’t miss these talks!
themodernsoftware.dev
Elephant in the room
Unlikely that we’ll be able to significantly increase enrollment cap (not my call)
If it were up to me everyone would be allowed in and we’d be teaching this class in the Stanford football stadium
If you’re #45 on the waitlist perhaps you’ll get lucky and people will be so turned off after this lecture that you get in
We’ll try to get a larger space 
All lecture materials on official course page
Additional reading will be there
Links to assignments
Joke
Who here has used AI coding tools?
ChatGPT?
Who has used ChatGPT to cheat on an assignment? [GET THEM!]
These are assignments are meant to be done using coding assignments
Assignments
Mainly in python and javascript
You should really have familiarity with this, so if you’re going to write messages in Ed asking for how to install python on your machine this isn’t the class for you
Quite a bit of software experience is necessary
Don’t worry too much about grades
Assignments are more about completion rather than correctness
Much of it will be about experimentation - I will give you parameters and invite you to explore 
Description of final project will go out later today
The students that are enrolled in the class will get access to many credits from different AI dev tool providers

How LLMs Work in 5 Slides 
(For Engineers)
themodernsoftware.dev

Basics
LLMs (large language models) are autoregressive models for next-token prediction

themodernsoftware.dev
Only math equation of the quarter. This equation explains why “context length” is such an important concept in LLMs (also to scare away the GSB students)

Basics
themodernsoftware.dev
a 
for 
loop 
for 
Embedding layer
0.3
the
0.6
idx
0.1
cat
Tokenize inputs using fixed vocabulary
Convert tokens into fixed-dimensional numerical vectors (~1-3K dimensions)
Transformers layers (12-96+) using self-attention mechanism (Viswani et. al. 2017)
Get probability distribution over most likely next token
Causal self-attention mechanism - internal mathematical mechanism that helps the model learn relationships between words, syntactically and semantically 
Tokenization is a bit more complicated (tokens that are embedded aren’t always full english words)
At the end every words is just a probability in a table

Training Process
Stage 1
Self-supervised pretraining
Teach the model notion of language on a variety of often public data sources
100s of billions to trillion+ tokens (language and code)
Common Crawl, Wikipedia, StackExchange, Public Github repos
Write a for loop → that could be used in a piece of code
Stage 2
Supervised finetuning
Teach model to follow instructions
High-quality, curated prompt-response pairs (“what is the capital of Croatia” -> “Zagreb is the capital”)
Tens of thousands to 100s of thousands of pairs 
Write a for loop → ok here’s a for loop…
Stage 3
Preferencing tuning
Align model outputs with human preferences (helpfulness, correctness, readability)
Collect pairs of outputs for same prompt and train reward model to predict preferred output
Tens of thousands to 100s of thousands of human-labeled comparisons
Write a for loop → for idx in range(10):
themodernsoftware.dev
GPT-3 (OpenAI)
Trained on ~300 billion tokens.
Sources included Common Crawl, WebText2, Books1/2, and Wikipedia.
Roughly 570 GB of clean text data after filtering and deduplication.
PaLM (Google, 2022)
Trained on 780 billion tokens.
Mixture of web documents, books, Wikipedia, and GitHub code.
LLaMA (Meta, 2023)
LLaMA-65B: 1.4 trillion tokens.
Mix of Common Crawl, GitHub, Wikipedia, ArXiv, StackExchange, and books.

 Code-Specific Models
Codex (OpenAI, 2021)
Based on GPT-3, further pretrained on tens of billions of code tokens from public GitHub repositories.
Exact size not disclosed, but estimates put it around 150+ GB of code.
Code LLaMA (Meta, 2023)
Built on LLaMA-2, trained on 500 billion code tokens.
Sources: GitHub, StackOverflow, coding tutorials, and filtered high-quality code corpora.
StarCoder (HuggingFace + ServiceNow, 2023)
Trained on 1 trillion tokens total, with a large fraction being over 600+ programming languages from The Stack dataset.
About 3.1 TB of source code after filtering.

For reference English wikipedia is about 3B tokens


Training Process
Reasoning models
Extend training with chain-of-thought reasoning traces
Tool-use integration
Get human preferences on reasoning steps
Reinforcement learning to learn how to evaluate reasoning traces, backtrack, etc
Size
GPT-3/Claude 3.5 Sonnet - 175B parameters
LLaMA 3.1 - 405B parameters
GPT-4 - 1.8T (reported)
themodernsoftware.dev
Reasoning models are typically the ones that say “-think” in the model name
LLMs are trained to have “special” capabilities like the ability to generate relevant functions

In practice
Strengths
Expert-level code completion
Code understanding
Code fixing
Limitations
Hallucinations
Generating non-existent/out-of-date APIs (mitigated with robust context engineering)
Context window limits
~100-200K tokens but not all are created equal
Latency
Seconds to minutes per request depending on task (plan and delegate accordingly)
Cost	
$1-3 per million input tokens, $10+ per million output tokens for best models
themodernsoftware.dev
Hallucinations seem more prominent in languages that are less well-represented in public code corpora
Context windows
primacy/recency bias (recency ends up still being strong)
Lost-in-the-middle effect
Latency
Thinking through how to organize your work so that you are maximizing your thinking utilization
A lot of human context switching
Still pretty expensive to run LLMs at scale but that cost drops by around 10x every year

themodernsoftware.dev
Questions?

